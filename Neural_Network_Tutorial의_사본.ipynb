{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "foqSUAYoAlVp",
        "8c0yeebaBEAH",
        "FHDNufM5BwO9",
        "yaCY1QD8_jzH",
        "JngCEQPgEMzq",
        "gwrq_xMLE4fM",
        "mreHJxPMJPZt",
        "QVMGUrCipHnP",
        "O38ZZijC4mmH",
        "fZlOvyOa7Ext",
        "cFHu6yz8UnW5"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiyoonJeong-Archive/Abnormal-EEG-Signal-Classification-Using-CNNs/blob/main/Neural_Network_Tutorial%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJEh9hhVP01k"
      },
      "source": [
        "<p align='center'><img src='https://github.com/jmasis/nntutorialmentaleffort2021/raw/main/Banner.jpg'\n",
        "alt=\"2 Layer Network\" height=175px width=auto/></p>\n",
        "\n",
        "# **2nd Workshop on Mental Effort**\n",
        "# Tutorial 2: An Introduction to Neural Network Modeling\n",
        "Sebastian Musslick & Javier Alejandro Masis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5dfiQNaRA89"
      },
      "source": [
        "## Outline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCb2uZr2QUHx"
      },
      "source": [
        "This notebook is composed of three sections, each introducing different concepts of neural network modeling.\n",
        "\n",
        "1.   In the **first section** we will train a **2-layer feed-forward network** to perform different logical operations (AND/OR/XOR). Using the **delta-rule**, we will teach the network to learn both the *AND* rule, as well as the *OR* rule. Finally, we will investigate the limitations of this network by training it on the non-linear *eXclusive OR (XOR)* rule.\n",
        "\n",
        "2.   In the **second section** we will train a **3-layer neural network** with a set of non-linear hidden units. We will use **backpropagation** to train the network and show that it is able to learn the XOR rule.\n",
        "<!-- We will also make an attempt to investigate the network's learned weights in order to understand it's learned solution. -->\n",
        "\n",
        "3.   ***BONUS:*** In the **third section**, we will build a **connectionist model of cognitive control** (Cohen, Dunbar & McClelland, 1990), and will train it to perform the Stroop task. We will then investigate the model's ability to reproduce the response congruency effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYZV3vOqQZmU"
      },
      "source": [
        "## Before we start: Let's import some packages.\n",
        "\n",
        "To execute the code block below, click on the little arrow symbol on the left side of the code block:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzDeE6sdP-eM"
      },
      "source": [
        "import numpy as np # for doing math easily in python\n",
        "import matplotlib.pyplot as plt # for plotting in python\n",
        "import seaborn as sns # for beautiful plots in python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWJciWz8qHXg"
      },
      "source": [
        "This might take a minute or two. The arrow should turn into a [1] once the code executed successfully and the packages are loaded. Let's now dive into our first neural network simulation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgbKp0CVqbbU"
      },
      "source": [
        "## Simulation 1: Two-Layer Neural Network and the Delta-Rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avCOpEtmyiBI"
      },
      "source": [
        "In this simulation we will train a 2-layer feed-forward network to perform different logical operations (AND/OR/XOR)&mdash;also called \"gates\"&mdash;based on two input variables. This section will introduce you to the delta rule, one of the most basic learning rules in neural networks. It will also demonstrate fundamental computational limitations of neural networks as famously pointed out by Minsky & Papert (1969)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFahCTI3QfHg"
      },
      "source": [
        "### Training Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2GoAL5uQEk_"
      },
      "source": [
        "Before we build the 2-layer network, we need to define what the network will be tasked with. All tasks provided to the network are expressed as patterns in the training environment. There are two types of patterns. <font color='#4C6EBC'>**Input patterns**</font> are provided to the input layer of the network. <font color='#B04E43'>**Training or output patterns**</font> represent the 'correct' response to the input patterns.\n",
        "\n",
        "We will define 3 different training environments that are based on the *boolean functions* AND, OR and XOR. The input patterns for each of the environments is the same. Each input pattern consists of 2 bits that are passed to the network. That is, the network takes as input two values that can be either 0 or 1. Each of the three boolean functions (AND/OR/XOR) defines how these two inputs are combined to a single binary output value.\n",
        "\n",
        "Boolean functions can be expressed in a *truth table*. Every row in a truth table represents a single input pattern with its corresponding output pattern. Below you will find the truth tables for each of the boolean functions that we will use for training.\n",
        "\n",
        "\n",
        "<br>\n",
        "<center>**AND**</center>\n",
        "\n",
        "| <font color='#4C6EBC'>Input 1 | <font color='#4C6EBC'>Input 2 | <font color='#B04E43'>Output</font> |\n",
        "|---------|---------|--------|\n",
        "| <font color='#4C6EBC'>0       | <font color='#4C6EBC'>0       | <font color='#B04E43'>0      |\n",
        "| <font color='#4C6EBC'>0       | <font color='#4C6EBC'>1       | <font color='#B04E43'>0</font>      |\n",
        "| <font color='#4C6EBC'>1       | <font color='#4C6EBC'>0       | <font color='#B04E43'>0</font>      |\n",
        "| <font color='#4C6EBC'>1       | <font color='#4C6EBC'>1       | <font color='#B04E43'>1</font>      |\n",
        "\n",
        "<br>\n",
        "<center>**OR**</center>\n",
        "\n",
        "| <font color='#4C6EBC'>Input 1 | <font color='#4C6EBC'>Input 2 | <font color='#B04E43'>Output</font> |\n",
        "|---------|---------|--------|\n",
        "| <font color='#4C6EBC'>0       | <font color='#4C6EBC'>0       | <font color='#B04E43'>0</font>      |\n",
        "| <font color='#4C6EBC'>0       | <font color='#4C6EBC'>1       | <font color='#B04E43'>1</font>      |\n",
        "| <font color='#4C6EBC'>1       | <font color='#4C6EBC'>0       | <font color='#B04E43'>1</font>      |\n",
        "| <font color='#4C6EBC'>1       | <font color='#4C6EBC'>1       | <font color='#B04E43'>1</font>      |\n",
        "\n",
        "\n",
        "<br>\n",
        "<center>**XOR**</center>\n",
        "\n",
        "| <font color='#4C6EBC'>Input 1 | <font color='#4C6EBC'>Input 2 | <font color='#B04E43'>Output</font> |\n",
        "|---------|---------|--------|\n",
        "| <font color='#4C6EBC'>0       | <font color='#4C6EBC'>0       | <font color='#B04E43'>0</font>      |\n",
        "| <font color='#4C6EBC'>0       | <font color='#4C6EBC'>1       | <font color='#B04E43'>1</font>      |\n",
        "| <font color='#4C6EBC'>1       | <font color='#4C6EBC'>0       | <font color='#B04E43'>1</font>      |\n",
        "| <font color='#4C6EBC'>1       | <font color='#4C6EBC'>1       | <font color='#B04E43'>0</font>      |\n",
        "\n",
        "Let's generate these input and output patterns by executing the following code block...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYizKsSbQPf7"
      },
      "source": [
        "# define input patterns (same across the different logical tasks)\n",
        "input_patterns = np.array([[ 0.,  0.],\n",
        "                           [ 0.,  1.],\n",
        "                           [ 1.,  0.],\n",
        "                           [ 1.,  1.]])\n",
        "\n",
        "# define output patterns for AND rule\n",
        "output_patterns_AND = np.array([[ 0.],\n",
        "                                [ 0.],\n",
        "                                [ 0.],\n",
        "                                [ 1.]])\n",
        "\n",
        "# define output patterns for OR rule\n",
        "output_patterns_OR = np.array([[ 0.],\n",
        "                               [ 1.],\n",
        "                               [ 1.],\n",
        "                               [ 1.]])\n",
        "\n",
        "output_patterns_XOR = np.array([[ 0.],\n",
        "                                [ 1.],\n",
        "                                [ 1.],\n",
        "                                [ 0.]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP4UU_F3RMii"
      },
      "source": [
        "### Network Training\n",
        "\n",
        "Now that we have the training data let's build the actual network. Our desired network looks like this:\n",
        "\n",
        "<p align='center'><img src='https://github.com/jmasis/nntutorialmentaleffort2021/blob/a47c4f5fc7eae409c60a63b317a5cce7f92948d6/2LNetwork.png?raw=True'\n",
        "alt=\"2 Layer Network\" width=400px height=auto/></p>\n",
        "\n",
        "<!-- ![picture](https://github.com/jmasis/nntutorialmentaleffort2021/blob/a47c4f5fc7eae409c60a63b317a5cce7f92948d6/2LNetwork.png?raw=True) -->\n",
        "\n",
        "The input layer of the network encompasses two units. These two input units project to the unit of the output layer. Training a neural network typically encompasses two steps. First, we present an input pattern to the network and let it compute the corresponding output pattern. We call this step **forward pass** since it passes information forward, from the input layer to the output layer. Second, we need to compare the output that the network produced with the true (correct) output. The resulting error is used to adjust the weights of the network, starting from the \"top\" (the output layer\"), and moving to the \"bottom\" (the input layer). We will call this step the **backward pass** since the error is passed backwards through the network, to adjust its weights.\n",
        "\n",
        "The function `train2LayerNetwork` defined below is used to train the network, by iterating between the forward pass and the backward pass. Your task will be to fill in the missing code in this function. The function will perform the following computations (steps marked in **bold** are the focus of the exercises).\n",
        "\n",
        "1) Initialization: This step involves initializing layers and weights\n",
        "\n",
        "2) **Forward Pass**: In this step we will compute the network's activity based on its input pattern and its weights.\n",
        "\n",
        "3) **Backward Pass**: In this step we will adjust the weights of the network based on the produced output patterns of the network and the feedback provided by the training patterns\n",
        "\n",
        "Steps 2) and 3) will be performed for each input pattern in each training iteration. Before we tinker with the code, let's discuss each step in detail, beginning with the forward pass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMEC-82QVQRl"
      },
      "source": [
        "#### Forward Pass ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53rM6UgNT2hB"
      },
      "source": [
        "\n",
        "In the forward pass we will propagate activity through the network, layer by layer. In feedforward networks, we already know the activity of the input layer as it corresponds to the input pattern. Therefore, We will begin with computing the activity of the units in the second layer.\n",
        "\n",
        "Let's say that the second layer has $N$ units. Let $y_j$\n",
        "be the activation of a unit in the second layer that we want to compute where $j \\in \\{1,...,N\\}$. The activation of a unit is a function of its net input. In neural network models of cognition, single units often represent populations of neurons. The higher the activity of a unit, the more neurons are active (responding) in the corresponding neural population. We are usually interested in **differentiable, monotonic functions**, such as the **logistic function** (see Excursion below):\n",
        "<br/><br/>\n",
        "\n",
        "\\begin{equation}\n",
        "y_j = \\textrm{logistic}(net_{y_j}) = \\frac{1}{1+e^{-net_{y_j}}}\n",
        "\\end{equation}\n",
        "\n",
        "<br/><br/>\n",
        "When we plot $y_j$ (the unit's activity) as a function of its net input, we get something like this:\n",
        "\n",
        "<p align='center'><img src='https://github.com/jmasis/nntutorialmentaleffort2021/blob/a47c4f5fc7eae409c60a63b317a5cce7f92948d6/2LSigmoid.png?raw=True'\n",
        "alt=\"2 Layer Network\" width=400px height=auto/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl5bSsABWZjx"
      },
      "source": [
        "##### **Excursion: What is a differentiable, monotonic function?**\n",
        "\n",
        "###### <ins>Differentiable</ins>\n",
        "A differentiable function is a function that has a derivative for every point in which it exists. For example, $f(x) = 2x$ is a differentiable function because its derivative $\\frac{d}{dx}f(x) = 2$ (its slope or rate of change) for every point in $x$.\n",
        "\n",
        "However, we can define a function\n",
        "\n",
        "$$\n",
        "g(x) = |x|\n",
        "$$\n",
        "\n",
        "that is defined for every point, but it is *not* differentiable because there is a discontinuity, or *jump*, in the derivative at $x = 0$.\n",
        "\n",
        "###### <ins>Monotonic</ins>\n",
        "\n",
        "A monotonic function, is a function that is always increasing, or always decreasing, but not changing directions, such as $f(x) = 2x$ or $f(x) = -2x$. Even our function $g(x)$ from above is also monotonic. .\n",
        "\n",
        "###### <ins>Differentiable versus Monotonic</ins>\n",
        "\n",
        "Is our non-differentiable function $g(x)$ from above monotonic?\n",
        "\n",
        "Is the function $f(x) = \\sin(x)$ monotonic? Is it differentiable?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2TnEHPyDx-A"
      },
      "source": [
        "##### <ins>*Thought question*</ins>\n",
        "*Why do we desire differentiable, monotonic activation functions?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foqSUAYoAlVp"
      },
      "source": [
        "###### <ins>*Answer*</ins>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-AB6thyAwj8"
      },
      "source": [
        "For technical reasons, we are usually interested in **differentiable** activation functions. This is because we need to differentiate these functions for the backward pass (see the next section). In addition, since we are interested in modeling processes of the human brain, we would like the activation function to be **monotonically** increasing. This means: the higher the input to the unit, the higher its activity (i.e. the stronger the response of the underlying neural population). Finally, we would like to constrain the activity of the unit. For instance, we wouldn't want to allow for activations below 0 (what does that even mean?), so we set a lower bound at 0 (0% activity). For convenience, we also would want to limit our activity to 1 (corresponding to 100% activity). Fortunately, the sigmoid function introduced above fits our needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLfqvfDaWUEK"
      },
      "source": [
        "##### **How do we integrate a unit's inputs?**\n",
        "\n",
        "The sigmoid activation function makes sure that the activation of a unit is bound between 0 and 1. However, in order to compute the activity $y_j$ we need to know the net input of a unit. The net input of unit $y_j$ is simply the sum of the activity of the sending units in the previous layer, weighted by their projection weights. Let's say that the sending, input layer $x$ has $M$ units. Then the net input $y_j$ of a unit in the receiving layer corresponds to the sum of the inputs from the sending layer, weighted by the weights that connect the sending layer with the receiving layer,\n",
        "\n",
        "\\begin{equation}\n",
        "net_{y_j} = \\sum_{i=1}^M x_i w_{j,i}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "where $x_i$ corresponds to the activity of input unit $i$ and $w_{j,i}$ corresponds to the weight of input unit $i$ to unit $j$ in the second layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dfzZ1wsfjc1"
      },
      "source": [
        "##### **Let's manually compute the activation of $y_{j}$**\n",
        "\n",
        "Now that we know the general rule to compute the activity of a neural network we can apply this to our example above. Say we want to compute the network's activity for the input pattern $[0,1]$. Since this is a very simple network, all we need to do is compute the activity of the unit $y_1$ in the second (output) layer.\n",
        "\n",
        "<p align='center'><img src='https://github.com/jmasis/nntutorialmentaleffort2021/blob/a47c4f5fc7eae409c60a63b317a5cce7f92948d6/2LNetwork.png?raw=True'\n",
        "alt=\"2 Layer Network\" width=200px height=auto/></p>\n",
        "\n",
        "<!-- Let's assume that the network has the weights -->\n",
        "\n",
        "<!-- $w_{1,1} = -0.5$\n",
        "\n",
        "$w_{1,2} = 2$.\n",
        "\n",
        "Since the input pattern is $[0,1]$, the the input layer units take on the following values:\n",
        "\n",
        "$x_1 = 1$\n",
        "\n",
        "$x_2 = 0$.\n",
        "\n",
        "Now we can compute the net input of $y_1$:\n",
        "\n",
        "$net_{y_1} = w_{1,1} x_1 + w_{1,2} x_2 = 0.5 * 1 + 2 * 0 = -0.5$\n",
        "\n",
        "Finally we can compute the activation of the unit for the input $[0,1]$:\n",
        "\n",
        "$y_1 = \\frac{1}{1+e^{-net_{y_1}}} = \\frac{1}{1+e^{-(-0.5)}} = 0.3775406688$ -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USWqyhdMcuzf"
      },
      "source": [
        "##### **Exercise 1.1**\n",
        "Fill in the missing parts needed to compute the output unit's net input, as well as its activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yete_cA0gF-K",
        "outputId": "a576281d-55d9-4139-dc40-6469612f2c46"
      },
      "source": [
        "## Let's assume that the network has the weights\n",
        "w_1_1 = -0.5\n",
        "w_1_2 = 2\n",
        "\n",
        "## Since the input pattern is [1,0],\n",
        "## the input layer units take on the following values\n",
        "x_1 = 1\n",
        "x_2 = 0\n",
        "\n",
        "## Now we can compute the net input of y_1:\n",
        "net_y_1 = x_1 * w_1_1 + x_2 * w_1_2\n",
        "print(f'net_y_1 = {net_y_1}')\n",
        "\n",
        "## Finally, we can compute the activation of y_1:\n",
        "y_1 = 1 / (1 + np.exp(-net_y_1))\n",
        "print(f'y_1 = {y_1}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "net_y_1 = -0.5\n",
            "y_1 = 0.3775406687981454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c0yeebaBEAH"
      },
      "source": [
        "###### **Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW1XeQFQBI-1",
        "outputId": "ce988b03-c17d-44cb-a9ed-ea3a693e932e"
      },
      "source": [
        "## Let's assume that the network has the weights\n",
        "w_1_1 = -0.5\n",
        "w_1_2 = 2\n",
        "\n",
        "## Since the input pattern is [0,1],\n",
        "## the input layer units take on the following values\n",
        "x_1 = 1\n",
        "x_2 = 0\n",
        "\n",
        "## Now we can compute the net input of y_1:\n",
        "net_y_1 = w_1_1*x_1 + w_1_2*x_2\n",
        "print(f'net_y_1 = {net_y_1}')\n",
        "\n",
        "## Finally, we can compute the activation of y_1:\n",
        "y_1 = 1 / (1 + np.exp(-net_y_1))\n",
        "print(f'y_1 = {y_1}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "net_y_1 = -0.5\n",
            "y_1 = 0.3775406687981454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i4zqG2SNn0Y"
      },
      "source": [
        "##### **Quick aside on notation!**\n",
        "\n",
        "Rather than defining the weights and input units separately (e.g. $w_{1,1}$ and $w_{1,2}$), we can define them more compactly as vectors (and eventually matrices) in order to make it easier to write out what is happening. If we do this, then we have the input vector $x$ and the weight vector $w$\n",
        "\n",
        "\\begin{equation}\n",
        "x = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, w = \\begin{bmatrix} -0.5 \\\\ 2 \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "The above sum takes two vectors and turns them into one number, a constant or scalar. That tells us that we can write it out as a **dot product**,\n",
        "\n",
        "\\begin{equation}\n",
        "net_{y_j} = \\sum_{i=1}^M x_i w_{j,i} = x \\cdot w\n",
        "\\end{equation}\n",
        "\n",
        "which can be written as the transpose of $x$ (horizontal/row vector instead of vertical/column vector) times $w$,\n",
        "\n",
        "\\begin{equation}\n",
        "net_{y_j} = x^{T}w = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\begin{bmatrix} -0.5 \\\\ 2 \\end{bmatrix}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abeTRtJzC_F6"
      },
      "source": [
        "##### **Exercise 1.2**\n",
        "Fill in the missing parts needed to compute the output unit's activation using vector computation, and see if you get the same answer. This is important because below we will use matrix format in our code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I-dcRxUQr0B",
        "outputId": "eddbcced-dda1-4149-8a4d-233b5f0bd2ca"
      },
      "source": [
        "## Define the x and w vectors\n",
        "w = np.array([[-0.5],\n",
        "              [2]])\n",
        "x = np.array([[1],\n",
        "              [0]])\n",
        "\n",
        "## Calculate the net activation of y using the dot product\n",
        "net_y_1 = np.dot(x.T, w)\n",
        "print(f'net_y_1 = {net_y_1[0][0]}')\n",
        "\n",
        "## Finally, we can compute the activation of y_1:\n",
        "y_1 = 1 / (1 + np.exp(-net_y_1))\n",
        "print(f'y_1 = {y_1[0][0]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "net_y_1 = -0.5\n",
            "y_1 = 0.3775406687981454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHDNufM5BwO9"
      },
      "source": [
        "###### **Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNepvOFNB1uq",
        "outputId": "0fb57fb8-0faf-45f1-c0ca-56670a0e142b"
      },
      "source": [
        "## Let's write it out in matrix format and see if we get the same answer.\n",
        "## This is important because below we will use matrix format in our code.\n",
        "\n",
        "## Define the x and w vectors\n",
        "w = np.array([[-0.5],\n",
        "              [2]])\n",
        "x = np.array([[1],\n",
        "              [0]])\n",
        "\n",
        "## Calculate the net activation of y using the dot product\n",
        "net_y_1 = np.dot(x.T,w) # alternative: x.T * w #\n",
        "print(f'net_y_1 = {net_y_1[0][0]}')\n",
        "\n",
        "## Finally, we can compute the activation of y_1:\n",
        "y_1 = 1 / (1 + np.exp(-net_y_1))\n",
        "print(f'y_1 = {y_1[0][0]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "net_y_1 = -0.5\n",
            "y_1 = 0.3775406687981454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSbyb17HfYNr"
      },
      "source": [
        "##### **What does this output mean?**\n",
        "\n",
        " It means that the network implements a function that maps the input pattern $[0,1]$ to the value $0.3775406688$. But remember, the network has not been trained yet, and we have run it with the equivalent of *random weights*. All we've checked so far, is that it *runs*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am9Pu9CofoCA"
      },
      "source": [
        "##### **Excursion: What if my network has more than two layers?**\n",
        "\n",
        "In a network with more than two layers, one would then proceed with computing the activation of the units in the third layer. This is done the same way as with the second layer: The activity of each unit in the third layer is some activation function of its net input. The net input is the weighted sum of the activities of the second layer where the weights correspond to the projection weights from the second to the third layer. In the feedforward pass one can apply this procedure layer by layer until the activity of the final (output) layer is computed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2e1u2oQDh-p"
      },
      "source": [
        "##### <ins>*Thought question*</ins>\n",
        "*In a network with more than 2 layers, could we calculate the forward pass all at once, or would we have to do it step by step? Why do you think so?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaCY1QD8_jzH"
      },
      "source": [
        "###### <ins>*Answer*</ins>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5EEs1mu_rem"
      },
      "source": [
        "We can compute the activtiy of the output layer all at once. To illustrate this, consider the simple network above. There, we can compute the output\n",
        "\n",
        "\\begin{equation}\n",
        "y = \\textrm{logistic}(net_{y}) = \\textrm{logistic}(x^T \\cdot w)\n",
        "\\end{equation}\n",
        "\n",
        "Now say we wanted to add another layer with an output unit $z$, and let $y$ project to $z$ through the weight $\\gamma$. Furthermore, we assume that the activation function of unit $z$ is also a logistic. Then we can compute the output of the new network as follows:\n",
        "\n",
        "\\begin{aligned}\n",
        "z &= \\textrm{logistic}(net_{z}) = \\textrm{logistic}(y^T \\cdot \\gamma) \\\\\n",
        "z &= \\textrm{logistic}(\\textrm{logistic}(net_{y})^T \\cdot \\gamma) \\\\\n",
        "z &= \\textrm{logistic}(\\textrm{logistic}(x \\cdot w)^T \\cdot \\gamma)\n",
        "\\end{aligned}\n",
        "\n",
        "The last line expresses the full computation of a three-layer network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhN0dVM_ceCc"
      },
      "source": [
        "##### **Exercise 1.3**\n",
        "\n",
        "Let's say we wanted the network to implement a function like the OR-rule.\n",
        "\n",
        "<br>\n",
        "<center>**OR**</center>\n",
        "\n",
        "| <font color='#4C6EBC'>Input 1 | <font color='#4C6EBC'>Input 2 | <font color='#B04E43'>Output</font> |\n",
        "|---------|---------|--------|\n",
        "| <font color='#4C6EBC'>0       | <font color='#4C6EBC'>0       | <font color='#B04E43'>0</font>      |\n",
        "| <font color='#4C6EBC'>0       | <font color='#4C6EBC'>1       | <font color='#B04E43'>1</font>      |\n",
        "| <font color='#4C6EBC'>1       | <font color='#4C6EBC'>0       | <font color='#B04E43'>1</font>      |\n",
        "| <font color='#4C6EBC'>1       | <font color='#4C6EBC'>1       | <font color='#B04E43'>1</font>      |\n",
        "\n",
        "\n",
        "In this case we would like it if the network produced an output $y \\approx 1$ for the input pattern [0, 1] (because $0$ OR $1$ = $1$). We could do this by changing the weights of the network in a smart way. Change the weights such that  $y \\approx 1$ for the following input patterns, reproducing the OR gate: <br/>\n",
        "[0, 1] --> 1<br/>\n",
        "[1, 0] --> 1<br/>\n",
        "[1, 1] --> 1<br/>\n",
        "\n",
        "Can we also make the network produce the right response for the input pattern [0,0]?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR2lLxMEeFuh",
        "outputId": "10a18b8a-f40a-469e-e577-f56b6ffdf6f2"
      },
      "source": [
        "## Define the x and w vectors\n",
        "w = np.array([[20],\n",
        "              [20]])\n",
        "x = np.array([[0],\n",
        "              [0]])\n",
        "\n",
        "## Calculate the net activation of y using the dot product\n",
        "net_y_1 = np.dot(x.T,w)\n",
        "print(f'net_y_1 = {net_y_1[0][0]}')\n",
        "\n",
        "## Finally, we can compute the activation of y_1:\n",
        "y_1 = 1 / (1 + np.exp(-net_y_1))\n",
        "print(f'y_1 = {y_1[0][0]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "net_y_1 = 0\n",
            "y_1 = 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JngCEQPgEMzq"
      },
      "source": [
        "###### **Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hva5KTQEOcH",
        "outputId": "a0c27d90-bc3c-4368-f82f-ad0c29b1c2b2"
      },
      "source": [
        "## Define the x and w vectors\n",
        "w = np.array([[10],   # change this value for w_1\n",
        "              [10]])     # change this value for w_2\n",
        "x = np.array([[1],\n",
        "              [0]])\n",
        "\n",
        "## Calculate the net activation of y using the dot product\n",
        "net_y_1 = np.dot(x.T,w)\n",
        "print(f'net_y_1 = {net_y_1[0][0]}')\n",
        "\n",
        "## Finally, we can compute the activation of y_1:\n",
        "y_1 = 1 / (1 + np.exp(-net_y_1))\n",
        "print(f'y_1 = {y_1[0][0]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "net_y_1 = 10\n",
            "y_1 = 0.9999546021312976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4elR41qikg2"
      },
      "source": [
        "#### Backward Pass ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA_dS6ibimJH"
      },
      "source": [
        "Our goal is to teach the network a particular function (e.g. the OR-rule). That is, we want it to produce the correct output (e.g. $[1]$) for a given input pattern (e.g. $[0,1]$). Let's assume that we already have a network with some  weights, like the ones from the feedforward pass. We know from the forward pass in the previous section that its output for the input pattern $[0,1]$ isn't very close to what we want ($[0.377...]$ versus $[1]$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fBG335xVtJX"
      },
      "source": [
        "##### **Squared Error**\n",
        "\n",
        "So let's teach the network by providing it with some feedback about how well it did. We will do this by computing the error $E_{y_{j}}$ of output unit $y_{j}$. One way to compute the error is by taking the squared difference between the output of the network and the correct training pattern for a given output unit $y_{j}$:\n",
        "\n",
        "<!-- (<ins>*Note*</ins>*: to avoid death by subscripts, we are abbreviating the output unit $y_{j}$ to just $j$*) -->\n",
        "\n",
        "\\begin{equation}\n",
        "E_{y_j} = \\frac{1}{2}(y_j - t_j)^2\n",
        "\\end{equation}\n",
        "\n",
        "where $t_j$ is the correct training output for unit $y_{j}$. Note that the squared error is scaled by $\\frac{1}{2}$. We do this to make the math work out more nicely below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCjLjM-OWKwD",
        "outputId": "fb7d3644-91a4-4f84-86e8-8881e4812753"
      },
      "source": [
        "## Compute the error\n",
        "t_1 = 1\n",
        "error_y_j = 0.5*np.square(y_1 - t_1)\n",
        "print(f'error_y_j = {error_y_j[0][0]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error_y_j = 0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS6rvx4lExGP"
      },
      "source": [
        "##### <ins>*Thought question*</ins>\n",
        "*Why would we want to square the error?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwrq_xMLE4fM"
      },
      "source": [
        "###### <ins>*Answer*</ins>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLZNr2-oFGGa"
      },
      "source": [
        "The squared error has two advantages. First, the error is always positive, irrespective of whether the output of the network is too small ($y_j < t_j$) or too large ($y_j > t_j$). Second, squaring the error penalizes small deviations less [e.g., $(0.9-1)^2=0.1^2=0.01$] compared to larger deviations [e.g., $(0.5-1)^2=0.5^2=0.25$]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-l5_OF-XEct"
      },
      "source": [
        "##### **Error as feedback**\n",
        "\n",
        "Now that we know how to calculate the network's error, we can then use the error as a feedback signal to adjust the network's weights.\n",
        "\n",
        "We know that the error is a function of the weights of the network, e.g. a function of $w_{1,1}$. This is beause we can write the output of the network as a function of its weights: $y = \\textrm{logistic}(net_{y}) = \\textrm{logistic}(x^T \\cdot w) = \\textrm{logistic} \\left( \\begin{bmatrix} x_1 & x_2 \\end{bmatrix} \\begin{bmatrix} w_{1,1} \\\\ w_{1,2} \\end{bmatrix} \\right).$\n",
        "\n",
        "Let's assume that this <font color=\"#C00000\">error function of $w_{1,1}$ </font> looks like the solid red line in the following plot:\n",
        "\n",
        "<p align='center'><img src='https://github.com/jmasis/nntutorialmentaleffort2021/blob/326079df67a7bfe602d30d6aee246b64f53b0f39/ErrorSurface.png?raw=True'\n",
        "alt=\"2 Layer Network\" width=600px height=auto/></p>\n",
        "\n",
        "where <font color=\"#000000\"> $w_{1,1}^t$ </font> is the current weight of the network  at time step $t$ (e.g. $-0.5$).\n",
        "\n",
        "Our goal is to minimize the error function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dllo4-IwFDce"
      },
      "source": [
        "##### **Optimal weights**\n",
        "\n",
        "The value <font color=\"#2F5597\">$w_{1,1}^*$</font> is the <font color=\"#2F5597\">optimal weight</font> of the network because it provides the minimum error.\n",
        "\n",
        "In order to minimize the error we want to change our current weight <font color=\"#000000\"> $w_{1,1}^t$ </font> so that it becomes <font color=\"#2F5597\">$w_{1,1}^*$</font>.\n",
        "\n",
        "What if we computed <font color=\"#2F5597\">$w_{1,1}^*$</font> directly by finding the minimum of the <font color=\"#C00000\">error function</font>? In principle, this is a great solution. However, there are two problems:\n",
        "\n",
        "\n",
        "1.   We don't know the error function, so we cannot compute its minimum directly.\n",
        "2.   The error function is indeed a function of $w_{1,1}$, but it is also a function of $w_{1,2}$. So we need to somehow take into account $w_{1,2}$ when optimizing $w_{1,1}$ and vice versa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-LJslaeGeVS"
      },
      "source": [
        "##### **Derivative of the error**\n",
        "\n",
        "We may not know the entire error function, meaning we cannot compute the optimal weight <font color=\"#2F5597\">$w_{1,1}^*$</font> directly, but we do know the small part of the error function around our current weight $w_{1,1}^t$. Knowing the local area means that we can calculate the <font color=\"7030A0\">derivative of the error with respect to $w_{1,1}^t$ given the current weights, i.e. $\\frac{\\partial E_{y_1}^t}{\\partial w_{1,1}^t}$</font>.\n",
        "\n",
        "If you remember from calculus, the derivative of a function at any point in time tells us the slope or rate of change of that function, which can graphically be represented as a line tangent to that point on our function (e.g. <font color=\"7030A0\">the dotted purple line</font>).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5-YnRE6JHd5"
      },
      "source": [
        "##### <ins>*Thought question*</ins>\n",
        "\n",
        "*Using the derivative $\\frac{\\partial E_{y_1}^t}{\\partial w_{1,1}^t}$ above, should we increase or decrease $w_{1,1}^t$ in order to decrease $E_{y_1}$?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mreHJxPMJPZt"
      },
      "source": [
        "###### <ins>*Answer*</ins>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QB2XZLoJ58H"
      },
      "source": [
        "The derivative derivative $\\frac{\\partial E_{y_1}^t}{\\partial w_{1,1}^t}$ amounts to the <font color=\"7030A0\">purple slope</font> in the figure which is negative (pointing downward). This means that the error decreases with increases in the weight. Thys, we need to increase $w_{1,1}^t$ in order to decrease the error $E_{y_1}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxe8F9btJB-7"
      },
      "source": [
        "##### **Using the derivative of the error to adjust the weights**\n",
        "\n",
        "In the example above, the derivative of the error function has a negative slope. This means that in order to minimize the error, we have to increase the weight $w_{1,1}^t$ by some small amount <font color=\"#00B050\">$\\Delta w_{1,1}^t$</font>. That is, our <font color=\"C55A11\">weight for the next time step $w_{1,1}^{t+1}$</font> is computed as\n",
        "\n",
        "\\begin{equation}\n",
        "w_{1,1}^{t+1} = \\underbrace{w_{1,1}^t}_\\text{current weight} + \\underbrace{\\Delta w_{1,1}^t}_\\text{weight change}\n",
        "\\end{equation}\n",
        "\n",
        "where\n",
        "\n",
        "\\begin{equation}\n",
        "\\Delta w_{1,1}^t = - \\underbrace{\\alpha}_\\text{learning rate} \\cdot \\underbrace{\\frac{\\partial E_{y_1}^t}{\\partial w_{1,1}^t}}_\\text{error slope}.\n",
        "\\end{equation}\n",
        "\n",
        "The *learnign rate* $\\alpha$ is a constant that defines the step size of the weight change (how much we want to change weight in the direction of the derivative). The weight change itself is dermined by the error slope, that is, the partial derivative of the error $E_{y_1}^t$ of the output unit $y_1$ with respect to the weight $w_{1,1}^t$ at time step t. The error slope tells us how changes in $w_{1,1}^t$ affect $E_{y_1}^t$. If $\\frac{\\partial E_{y_1}^t}{\\partial w_{1,1}^t}$ is positive, then an increase in $w$ will increase $E_{y_1}^t$. Since we want to *decrease* the error, we add a minus sign in front of the term. This means that we want to change $w_{1,1}^t$ in the direction that *reduces* the error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ4_F5IKpA-_"
      },
      "source": [
        "##### <ins>*Thought question*</ins>\n",
        "\n",
        "*Why would we want to introduce a learning rate $\\alpha$? Would we want $\\alpha$ to be big or small?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVMGUrCipHnP"
      },
      "source": [
        "###### <ins>*Answer*</ins>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VIVUmyOpLcc"
      },
      "source": [
        "The learning rate $\\alpha$ allows us to *regulate* the step size with which we can adjust the error. Why do we need to adjust the step size? Well, imagine taking a step in the direction of the derivative in the Figure above. If we make this step too small, we are only moving a tiny amount to reduce the error, so it can take a long time until we reach the minimum. Alternatively, if we make this step too big then we may go too far and skip the minimum (note that the error increases after <font color=\"#2F5597\">$w_{1,1}^*$</font>). Thus, we want to use learning rate that is small enough so we don't skip the minimum, but that is large enough so we can make sufficient progress in one training step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTqWoRMYK-wI"
      },
      "source": [
        "##### **Expressing the error as a function of the current weights**\n",
        "\n",
        "If you recall from above, we reasoned that the error is a function of the weights because:\n",
        "\n",
        "1.   The error of unit $y_{j}$,\n",
        "\\begin{equation}\n",
        "E_{y_j} = \\frac{1}{2}(y_j - t_j)^2,  \n",
        "\\end{equation}\n",
        "is a function of the activity of $y_{j}$,\n",
        "\\begin{equation}\n",
        "y_j = \\frac{1}{1+e^{-net_{y_j}}}\n",
        "\\end{equation}\n",
        "therefore,\n",
        "\\begin{equation}\n",
        "E_{y_j} = \\frac{1}{2}\\left(\\frac{1}{1+e^{-net_{y_j}}} - t_j\\right)^2  \n",
        "\\end{equation}\n",
        "\n",
        "2.   The activity of $y_{j}$ is a function of its net activation\n",
        "\\begin{equation}\n",
        "net_{y_j} = \\sum_{i=1}^M x_i w_{j,i}\n",
        "\\end{equation}\n",
        "therefore, we can express the error function as\n",
        "\\begin{equation}\n",
        "E_{y_j} = \\frac{1}{2}\\left(\\frac{1}{1+e^{\\sum_{i=1}^M x_i w_{j,i}}} - t_j\\right)^2  \n",
        "\\end{equation}\n",
        "\n",
        "Now, this equation is a lot to handle, but it shows us how the error function really is a function of the weights, this means that we can now take the derivative of the error function with respect to the weights!\n",
        "\n",
        "Because the full expression of the error function is complex and would be hard to differentiate, we will take advantage of the fact there the error is simply a composition of simpler nested functions (all the ones we mentioned right above!).\n",
        "\n",
        "This nesting looks as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "E_{y_1}^t(w_{1,1}^t) = E_{y_1}(y_1(net_{y_1}(w_{1,1}^t)))\n",
        "\\end{equation}\n",
        "\n",
        "or in words: The error $E_{y_1}$ is a function of the output activation $y_1$ which is a functin of its net input $net_{y_1}$ which is a function of its weight $w_{1,1}^t$ (among other variables)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrwfmN56Og8R"
      },
      "source": [
        "##### **Back to high school: Love/hate relationship with something called the chain rule**\n",
        "\n",
        "To differentiate the error with respect to the weights, we will use something called the **chain rule** from calculus. The chain rule tells us that the derivative of a function of functions is equal to the product of the partial derivatives of all of those functions. Let's say we want to compute the partial derivative of the following function $f$ with respect to x:\n",
        "\n",
        "\\begin{equation}\n",
        "f(x) = (2x+y)^2\n",
        "\\end{equation}\n",
        "\n",
        "Note that we can rewrite $f(x)$ as a function of another function $g$:\n",
        "\n",
        "\\begin{equation}\n",
        "f(x) = g(x)^2\n",
        "\\end{equation}\n",
        "\n",
        "where\n",
        "\n",
        "\\begin{equation}\n",
        "g(x) = 5x+y, f(x) = x^2\n",
        "\\end{equation}\n",
        "\n",
        "To compute the partial derivative of $f$ with respect to $x$, i.e. $\\frac{\\partial f}{\\partial x}$, we can apply the chain rule:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial f}{\\partial x} = \\underbrace{\\frac{\\partial f}{\\partial g}}_\\text{derivative 1} \\cdot \\underbrace{\\frac{\\partial g}{\\partial x}}_\\text{derivative 2}\n",
        "\\end{equation}\n",
        "\n",
        "OK, let's decompose this.\n",
        "\n",
        "The first term $\\frac{\\partial f}{\\partial g}$ is the derivative of f with respect to g, amounting to $\\frac{\\partial f}{\\partial g}=2g(x)$.\n",
        "\n",
        "The second term $\\frac{\\partial g}{\\partial x}$ is the derivative of g with respect to x, amounting to $\\frac{\\partial g}{\\partial x}=5$.\n",
        "\n",
        "If we insert the second and the first term into the equation above, we get:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial f}{\\partial x} = 2g(x) \\cdot 5 = 10 (5x+y) = 50x + 10y\n",
        "\\end{equation}\n",
        "\n",
        "That wasn't so difficult. Now, what happens if we include one more nested function, e.g.,\n",
        "\n",
        "\\begin{equation}\n",
        "f(x) = g(h(x))^2?\n",
        "\\end{equation}\n",
        "\n",
        "In this case, we just iteratively apply the chain rule, and work our way from the most outer function to the most inner function:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial f}{\\partial x} = \\underbrace{\\frac{\\partial f}{\\partial g}}_\\text{derivative 1} \\cdot \\underbrace{\\frac{\\partial g}{\\partial h}}_\\text{derivative 2} \\cdot \\underbrace{\\frac{\\partial h}{\\partial x}}_\\text{derivative 3}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM5PtSwm1FmI"
      },
      "source": [
        "##### **Computing the error gradient**\n",
        "\n",
        "Now that we understand the principle of the chain rule, we can apply it to our nested error function from above:\n",
        "\n",
        "\\begin{equation}\n",
        "E_{y_1}^t(w_{1,1}^t) = E_{y_1}(y_1(net_{y_1}(w_{1,1}^t))).\n",
        "\\end{equation}\n",
        "\n",
        "Let's apply the chain rule to compute the error with respect to the weight:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial E_{y_1}^t}{\\partial w_{1,1}^t} = \\underbrace{\\frac{\\partial E_{y_1}^t}{\\partial y_1}}_\\text{derivative 1} \\quad\n",
        "\\underbrace{\\frac{\\partial y_1}{\\partial net_{y_1}}}_\\text{derivative 2} \\quad\n",
        "\\underbrace{\\frac{\\partial net_{y_1}}{\\partial w_{1,1}^t}}_\\text{derivative 3}\n",
        "\\end{equation}\n",
        "\n",
        "The first partial derivative $\\frac{\\partial E_{y_1}^t}{\\partial y_1}$ is easy to compute:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial E_{y_1}^t}{\\partial y_1} =  \\frac{\\partial \\left( \\frac{1}{2} (y_1 - t_1)^2 \\right)}{\\partial y_1} = (y_1 - t_1)\n",
        "\\end{equation}\n",
        "\n",
        "The second partial derivative $\\frac{\\partial y_1}{\\partial net_{y_1}}$ is a bit more complicated since we are dealing with a sigmoidal activation function $y_1 = 1/(1+e^{-net_{y_1}})$. However, it turns out that it's derivate can be simply computed as a function of $y_1$:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial y_1}{\\partial net_{y_1}} = y_1 (1 - y_1)\n",
        "\\end{equation}\n",
        "\n",
        "Finally, we compute the third partial derivative $\\frac{\\partial net_{y_1}}{\\partial w_{1,1}^t}$. Recall that $net_{y_1} = w_{1,1}^t x_1 + w_{1,2}^t x_2$. Thus, $\\frac{\\partial net_{y_1}}{\\partial w_{1,1}^t}$ amounts to\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial net_{y_1}}{\\partial w_{1,1}^t} =  x_1\n",
        "\\end{equation}\n",
        "\n",
        "Now let's put all pieces together in order to compute the final weight change for $w_{1,1}$:\n",
        "\n",
        "\\begin{align}\n",
        "\\Delta w_{1,1}^t = - &\\alpha \\cdot \\frac{\\partial E_{y_1}^t}{\\partial w_{1,1}^t} \\\\\n",
        "\\Delta w_{1,1}^t = - &\\alpha \\cdot \\underbrace{(y_1 - t_1)}_{\\frac{\\partial E_{y_1}^t}{\\partial y_1}} \\underbrace{y_1 (1 - y_1)}_{\\frac{\\partial y_1}{\\partial net_{y_1}}} \\underbrace{x_1}_{\\frac{\\partial net_{y_1}}{\\partial w_{1,1}^t}}\n",
        "\\end{align}\n",
        "\n",
        "A similar update rule can be applied for the other weight $w_{1,2}$.\n",
        "\n",
        "\\begin{align}\n",
        "\\Delta w_{1,2}^t = - &\\alpha \\cdot \\frac{\\partial E_{y_1}^t}{\\partial w_{1,2}^t} \\\\\n",
        "\\Delta w_{1,2}^t = - &\\alpha \\cdot (y_1 - t_1) y_1 (1 - y_1) x_2\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eisVBZyZO1tV"
      },
      "source": [
        "##### **Additional remarks: delta rule** ####\n",
        "Note that if we use a linear activation function, e.g. $y_j = m \\cdot net_{y_j} + n$, then the weight change reduces to\n",
        "\n",
        "\\begin{equation}\n",
        "\\Delta w_{j,i}^t = - \\alpha (y_j - t_j) x_i\n",
        "\\end{equation}\n",
        "\n",
        "where the constant $m$ gets absorbed into the learning rate $\\alpha$. This weight update rule is called the **delta rule**. Why is it called delta rule? That's because the weight adjustment is proportional to the difference (delta) between the actual output of the network $y_j$ and the correct output determined by the training pattern $t_j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7yWfDtu-iTk"
      },
      "source": [
        "### Exercise 1.4\n",
        "\n",
        "The function `train2LayerNetwork` initializes a simple 2-layer neural network with 2 input units and 1 output unit (like in the example above). Your task is to fill in the missing code for the computation of its output (forward pass), as well as the missing code for network training (backward pass). The missing code is marked with '`...`'.\n",
        "\n",
        "Once you completed the code, run the simulation below. The simulation will output for each learning epoch (1 forward pass and 1 backward pass on every training pattern) the mean squared error across all training patterns. Try to train the network on the two different training patterns (OR, AND).\n",
        "\n",
        "*Heads-up*: The network may not perform as well as on the AND pattern. Why do you think does the network not learn the AND task as well? You may set the debug variable to true for a more detailed output of the weight adjustments for each pattern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfSDqRk-cp7e"
      },
      "source": [
        "#### Define function 'train2LayerNetwork'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA9T7EQe-g_-"
      },
      "source": [
        "def train2LayerNetwork(input_patterns, output_patterns, learning_rate, MSE_threshold, num_epochs, train_bias=False,\n",
        "                       debug=False):\n",
        "    '''\n",
        "    inputs:\n",
        "      input_patterns: array of shape (4, 2)\n",
        "      output_patterns: array of shape (4, 1)\n",
        "      learning_rate: scalar,\n",
        "      MSE_threshold: scalar, the training stops once the MSE is below this value\n",
        "      num_epochs: scalar, maximum number of epochs to train\n",
        "      train_bias: boolean, if True includes terms allowing bias unit in network\n",
        "      debug: boolean, if True includes print statements to debug\n",
        "    output:\n",
        "      MSE_log: array of shape (1, num_epochs) showing change in network error over training\n",
        "    '''\n",
        "    ### network initialization ###\n",
        "\n",
        "    # let's define the number of input and output units as a function of the dimension of the input and output patterns respectively\n",
        "    NInputUnits = input_patterns.shape[1]\n",
        "    NOutputUnits = output_patterns.shape[1]\n",
        "\n",
        "    # let's also keep a running log of the error of the network\n",
        "    MSE_log = np.zeros((1, num_epochs))\n",
        "\n",
        "    # let's initialize the weights between the two layers with small random values, uniformly sampled between 0 and 0.1;\n",
        "    # the weight matrix will have as many (rows, columns) as (units in the input layer, units in the output layer)\n",
        "    w = np.random.uniform(0, 0.1, (NInputUnits, NOutputUnits))\n",
        "    if train_bias:\n",
        "        # we will also randomly initialize the bias weights to the output layer\n",
        "        bias = np.random.uniform(0, 0.1, (1, NOutputUnits))\n",
        "\n",
        "    ### network training ###\n",
        "\n",
        "    # the network will be trained in epochs.\n",
        "    for epoch in range(num_epochs):\n",
        "        # initialize mean squared error log for all patterns\n",
        "        MSE_patterns = np.zeros((output_patterns.shape[0],))\n",
        "\n",
        "        # within each training epoch, we will loop through every training pattern.\n",
        "        for pattern in range(input_patterns.shape[0]):\n",
        "\n",
        "            # FORWARD PASS #\n",
        "\n",
        "            # assign values to input layer\n",
        "            x = np.empty((NInputUnits,NOutputUnits))\n",
        "            x[:,0] = input_patterns[pattern,] # x will be a column vector\n",
        "\n",
        "            # compute net input of output layer using np.dot and x.T\n",
        "            y_net = np.dot(x.T, w)\n",
        "            if train_bias:\n",
        "                y_net = y_net + 1 * bias\n",
        "\n",
        "            # compute activation of output layer using sigmoidal activation function using np.exp\n",
        "            y = 1 / (1 + np.exp(-y_net))\n",
        "\n",
        "            # ERROR BACKPROPAGATION #\n",
        "\n",
        "            # compute the mean squared error of the output with respect to the correct training pattern\n",
        "            t = output_patterns[pattern,]\n",
        "            MSE_patterns[pattern] = 0.5 * (y - t) ** 2\n",
        "\n",
        "            # compute derivative of the error (0.5 * (y - t) ** 2) with respect to the output unit activation\n",
        "            dError_dAct =  (y - t)\n",
        "\n",
        "            # compute derivative of output unit activation with respect to its net input.\n",
        "            # Hint:  the derivative of a logistic function y(x) = 1/(1+exp(-x))\n",
        "            # is dy/dx = y * (1 - y)\n",
        "            dAct_dNet = y * (1 - y)\n",
        "\n",
        "            # compute the derivative of the net input (y_net = x*w) of the output layer with respect to its weights w\n",
        "            # note: the correct derivative should be a column vector\n",
        "            dNet_dw = x\n",
        "\n",
        "            if train_bias:\n",
        "                # compute the derivative of the net input of the output layer with respect to the bias unit\n",
        "                dNet_dBias = 1\n",
        "\n",
        "            # compute weight adjustment using dError_dAct, dAct_dNet and dNet_dw\n",
        "            delta_w = dError_dAct * dAct_dNet * dNet_dw\n",
        "            if train_bias:\n",
        "                delta_bias = dError_dAct * dAct_dNet * dNet_dBias\n",
        "\n",
        "            # adjust weights based on learning rate\n",
        "            w = w - learning_rate * delta_w\n",
        "            if train_bias:\n",
        "                bias = bias - learning_rate * delta_bias\n",
        "\n",
        "            # For debugging\n",
        "            if (debug):\n",
        "                print('----------')\n",
        "                print(f'pattern: {x}')\n",
        "                print(f'weights: {w}')\n",
        "                print(f'output: {y}')\n",
        "                print(f'MSE - output pattern: {MSE_patterns[pattern]} - {output_patterns[pattern,]}')\n",
        "                print(f'dError: {dError_dAct}')\n",
        "                print(f'dAct_dNet: {dAct_dNet}')\n",
        "                print(f'weight adjustment: {- learning_rate * delta_w}')\n",
        "\n",
        "        # log mean squared error for current epoch\n",
        "        MSE_log[0, epoch] = np.sum(MSE_patterns) / MSE_patterns.size\n",
        "\n",
        "        # break if we error threshold is reached\n",
        "        if MSE_log[0, epoch] < MSE_threshold:\n",
        "            break\n",
        "\n",
        "    # plot the error function\n",
        "    plt.plot(MSE_log[0,], \"b-\")\n",
        "    plt.xlim(0, num_epochs)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylim(0.04, 0.3)\n",
        "    plt.yticks([0.05, 0.1, 0.15, 0.2, 0.25])\n",
        "    plt.ylabel('MSE')\n",
        "    plt.title('MSE as a function of training epoch')\n",
        "    sns.despine(trim=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "981nh-nnASqA"
      },
      "source": [
        "#### Run Simulation\n",
        "\n",
        "We will start with defining critical simulation parameters:\n",
        "- `learning_rate` corresponds to the stepsize for each weight change\n",
        "- `MSE_threshold` defines the mean-squared error at which training is stopped\n",
        "- `max_num_epochs` corresponds to the maximum number of training iterations (in case MSE_threshold is not reached)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "tVKQv2JbAYdy",
        "outputId": "121e60d0-645f-48ed-d1a6-96398b4a2d7f"
      },
      "source": [
        "# define parameters\n",
        "input = input_patterns\n",
        "output_pattern = output_patterns_XOR # output_patterns_AND # output_patterns_XOR\n",
        "learning_rate = 0.3 # learning rate\n",
        "MSE_threshold = 0.001 # the training stops once the MSE is below this value\n",
        "max_num_epochs = 500 # maximum number of epochs to train\n",
        "train_bias = True # if True includes terms allowing bias unit in network\n",
        "debug = False # set this to True to print relevant variables at each epoch.\n",
        "\n",
        "# run the network\n",
        "train2LayerNetwork(input, output_pattern, learning_rate, MSE_threshold, max_num_epochs, train_bias, debug)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O38ZZijC4mmH"
      },
      "source": [
        "#### **Solution (Exercises 1.4 and 1.5)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dawptzgs4pLL"
      },
      "source": [
        "def train2LayerNetwork(input_patterns, output_patterns, learning_rate, MSE_threshold, num_epochs, train_bias=False,\n",
        "                       debug=False):\n",
        "    '''\n",
        "    inputs:\n",
        "      input_patterns: array of shape (4, 2)\n",
        "      output_patterns: array of shape (4, 1)\n",
        "      learning_rate: scalar,\n",
        "      MSE_threshold: scalar, the training stops once the MSE is below this value\n",
        "      num_epochs: scalar, maximum number of epochs to train\n",
        "      train_bias: boolean, if True includes terms allowing bias unit in network\n",
        "      debug: boolean, if True includes print statements to debug\n",
        "    output:\n",
        "      MSE_log: array of shape (1, num_epochs) showing change in network error over training\n",
        "    '''\n",
        "    ### network initialization ###\n",
        "\n",
        "    # let's define the number of input and output units as a function of the dimension of the input and output patterns respectively\n",
        "    NInputUnits = input_patterns.shape[1]\n",
        "    NOutputUnits = output_patterns.shape[1]\n",
        "\n",
        "    # let's also keep a running log of the error of the network\n",
        "    MSE_log = np.zeros((1, num_epochs))\n",
        "\n",
        "    # let's initialize the weights between the two layers with small random values, uniformly sampled between 0 and 0.1;\n",
        "    # the weight matrix will have as many (rows, columns) as (units in the input layer, units in the output layer)\n",
        "    w = np.random.uniform(0, 0.1, (NInputUnits, NOutputUnits))\n",
        "    if train_bias:\n",
        "        # we will also randomly initialize the bias weights to the output layer\n",
        "        bias = np.random.uniform(0, 0.1, (1, NOutputUnits))\n",
        "\n",
        "    ### network training ###\n",
        "\n",
        "    # the network will be trained in epochs.\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # initialize mean squared error log for all patterns\n",
        "        MSE_patterns = np.zeros((output_patterns.shape[0],))\n",
        "\n",
        "        # within each training epoch, we will loop through every training pattern.\n",
        "        for pattern in range(input_patterns.shape[0]):\n",
        "\n",
        "            # FORWARD PASS #\n",
        "\n",
        "            # assign values to input layer\n",
        "            x = np.empty((2,1))\n",
        "            x[:,0] = input_patterns[pattern,] # x will be a column vector\n",
        "\n",
        "            # compute net input of output layer using np.dot and x.T\n",
        "            y_net = np.dot(x.T, w)\n",
        "            if train_bias:\n",
        "                y_net = y_net + 1 * bias\n",
        "\n",
        "            # compute activation of output layer using sigmoidal activation function\n",
        "            y = 1 / (np.exp(- (y_net)) + 1)\n",
        "\n",
        "            # ERROR BACKPROPAGATION #\n",
        "\n",
        "            # compute the mean squared error of the output with respect to the correct training pattern\n",
        "            t = output_patterns[pattern,]\n",
        "            MSE_patterns[pattern] = (y - t) ** 2\n",
        "\n",
        "            # compute derivative of the error ((y - t) ** 2) with respect to the output unit activation\n",
        "            dError_dAct = (y - t)\n",
        "\n",
        "            # compute derivative of output unit activation with respect to its net input.\n",
        "            # Note that the derivative of a sigmoidal function y(x) = 1/(1+exp(-x))\n",
        "            # is dy/dx = y(x) * (1-y(x))\n",
        "            dAct_dNet = y * (1 - y)\n",
        "\n",
        "            # compute the derivative of the net input of the output layer with respect to its weights\n",
        "            # note: make sure that this derivative is formatted as a column vector\n",
        "            dNet_dw = x\n",
        "\n",
        "            if train_bias:\n",
        "                # compute the derivative of the net input of the output layer with respect to the bias unit\n",
        "                dNet_dBias = 1\n",
        "\n",
        "            # compute weight adjustment using dError_dAct, dAct_dNet and dNet_dw\n",
        "            delta_w = dError_dAct * dAct_dNet * dNet_dw\n",
        "            if train_bias:\n",
        "                delta_bias = dError_dAct * dAct_dNet * dNet_dBias\n",
        "\n",
        "            # For debugging\n",
        "            if (debug):\n",
        "                print('----------')\n",
        "                print(f'pattern: {x}')\n",
        "                print(f'weights: {w}')\n",
        "                print(f'output: {y}')\n",
        "                print(f'MSE - output pattern: {MSE_patterns[pattern]} - {output_patterns[pattern,]}')\n",
        "                print(f'dError: {dError_dAct}')\n",
        "                print(f'dAct_dNet: {dAct_dNet}')\n",
        "                print(f'weight adjustment: {- learning_rate * delta_w}')\n",
        "\n",
        "            # adjust weights based on learning rate\n",
        "            w = w - learning_rate * delta_w\n",
        "            if train_bias:\n",
        "                bias = bias - learning_rate * delta_bias\n",
        "\n",
        "        # log mean squared error for current epoch\n",
        "        MSE_log[0, epoch] = np.sum(MSE_patterns) / MSE_patterns.size\n",
        "\n",
        "        # break if we error threshold is reached\n",
        "        if MSE_log[0, epoch] < MSE_threshold:\n",
        "            break\n",
        "\n",
        "    # plot the error function\n",
        "    plt.plot(MSE_log[0,], \"b-\")\n",
        "    plt.xlim(0, num_epochs)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylim(0.04, 0.3)\n",
        "    plt.yticks([0.05, 0.1, 0.15, 0.2, 0.25])\n",
        "    plt.ylabel('MSE')\n",
        "    plt.title('MSE as a function of training epoch')\n",
        "    sns.despine(trim=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zhYsFx98Z7a"
      },
      "source": [
        "**Answer Exercise 1.4**\n",
        "\n",
        "*   The network fails to learn the AND task because, without a bias unit, it can never produce zero as output, e.g., for the input pattern [0, 0]. This is because, no matter how you set the weights, the net input for the input pattern [0, 0] will always be zero (the weights are multiplied with zero). Thus, the lowest activation that the network can achieve is logistic(0) = 0.5. To correct that, we need to include a trainable offset (bias) in the activation function (see next section)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQHtsEj5E0jj"
      },
      "source": [
        "\n",
        "**Answers Exercise 1.5**\n",
        "*   The bias adjustment corresponds to\n",
        "\\begin{equation}\n",
        "\\Delta w_{j,b}^t = - \\underbrace{\\alpha}_\\text{learning rate} \\cdot\\underbrace{\\frac{\\partial E_{y_1}^t}{\\partial w_{j,b}^t}}_\\text{derivative w.r.t. bias}\n",
        "\\end{equation}\n",
        "So \"all\" we need to do is figure out how to compute $\\frac{\\partial E_{y_1}^t}{\\partial w_{j,b}^t}$. Let's use the chain rule, the same way we did it when we computed the weight adjustments:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial E_{y_1}^t}{\\partial w_{j,b}^t} = \\frac{\\partial E_{y_1}^t}{\\partial y_{1}} \\cdot \\frac{\\partial y_{1}}{\\partial net_{y_1}} \\cdot \\frac{\\partial net_{y_1}}{\\partial w_{j,b}^t}\n",
        "\\end{equation}\n",
        "\n",
        "We know the first two terms from our calculations above:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial E_{y_1}^t}{\\partial y_{1}} = (y_1 - t_1)\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\\frac{\\partial y_{1}}{\\partial net_{y_1}} = y_1 (1 - y_1)\n",
        "\\end{equation}\n",
        "\n",
        "Now, all that's left is the derrivative of the net input with respect to the bias, $\\frac{\\partial net_{y_1}}{\\partial w_{j,b}^t}$. So let's take a look at the net input:\n",
        "\n",
        "\\begin{equation}\n",
        "net_{y_1} = w_{1,1} x_1 + w_{1,2} x_2 + \\underbrace{b_{y_j} w_{j,b}^t}_\\text{bias term}\n",
        "\\end{equation}\n",
        "\n",
        "Below, we discuss that we treat the bias unit as an input unit that is always set to 1 (it's always turned on), so the equation simplifies to\n",
        "\n",
        "\\begin{equation}\n",
        "net_{y_1} = w_{1,1} x_1 + w_{1,2} x_2 + 1 \\cdot w_{j,b}^t\n",
        "\\end{equation}\n",
        "\n",
        "This means that $\\frac{\\partial net_{y_1}}{\\partial w_{j,b}^t} = 1$. Now we can compute the entire gradient above:\n",
        "\n",
        "\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial E_{y_1}^t}{\\partial w_{j,b}^t} &= \\frac{\\partial E_{y_1}^t}{\\partial y_{1}} \\cdot \\frac{\\partial y_{1}}{\\partial net_{y_1}} \\cdot \\frac{\\partial net_{y_1}}{\\partial w_{j,b}^t} \\\\\n",
        "\\frac{\\partial E_{y_1}^t}{\\partial w_{j,b}^t} &= (y_1 - t_1) \\cdot y_1 (1 - y_1) \\cdot 1\n",
        "\\end{aligned}\n",
        "\n",
        "This results in the following weight adjustment:\n",
        "\n",
        "\\begin{aligned}\n",
        "\\Delta w_{j,b}^t &= - \\alpha \\cdot \\frac{\\partial E_{y_1}^t}{\\partial w_{j,b}^t} \\\\\n",
        "\\Delta w_{j,b}^t &= - \\alpha \\cdot  (y_1 - t_1) \\cdot y_1 (1 - y_1) \\cdot 1\n",
        "\\end{aligned}\n",
        "\n",
        "*   Implementing a trainable bias should improve the network's performance on the AND, as well as the OR rule. However, it should still not be able to learn the XOR rule.\n",
        "\n",
        "*   Learning the XOR rule requires a non-linear transformation of the input patterns, before the output layer. To accomplish this, we can include a non-linear \"hidden layer\" between the input and output layers of the network. This hidden layer will help the network learn a non-linear transformation of the input patterns, required to perform the XOR rule.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snpl1GbGUNzt"
      },
      "source": [
        "### The Role of Bias Units"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y63vAyW6RmnK"
      },
      "source": [
        "\n",
        "\n",
        "All three boolean functions (AND, OR, XOR) have something in common: They require to map the inputs $[0,0]$ to the output $[0]$. Can our neural network actually learn this mapping? Let's compute the network's output in response to the input pattern $x_1=0, x_2=0$ for any given weights $w_{1,1}, w_{1,2}$:\n",
        "\n",
        "$net_{y_1} = w_{1,1} x_1 + w_{1,2} x_2 = 0$\n",
        "\n",
        "$y_1 = \\frac{1}{1+e^{(-net_{y_1})}} = \\frac{1}{1+e^{(-0)}} = 0.5$.\n",
        "\n",
        "It turns out that, no matter how we set the weights $w_{1,1}, w_{1,2}$, the network's output will always be 0.5 for the input pattern [0, 0]. This is because the inputs $x_1=0, x_2=0$ lead to $net_{y_1} = 0$, which in turn yields an activation of $y_1 = 0.5$ due to the sigmoidal activation function:\n",
        "\n",
        "<p align='center'><img src='https://github.com/jmasis/nntutorialmentaleffort2021/blob/a47c4f5fc7eae409c60a63b317a5cce7f92948d6/2LSigmoid.png?raw=True'\n",
        "alt=\"2 Layer Network\" width=400px height=auto/></p>\n",
        "\n",
        "What if we could shift the sigmoidal activation function to the right? Then a net input of 0 would yield an activation close to 0.\n",
        "\n",
        "<p align='center'><img src='https://github.com/jmasis/nntutorialmentaleffort2021/blob/326079df67a7bfe602d30d6aee246b64f53b0f39/BiasEffect.png?raw=True'\n",
        "alt=\"Effect of bias on sigmoidal activation function\" width=400px height=auto/></p>\n",
        "\n",
        "Shifting the sigmoidal activation function corresponds to adding a negative bias term to the net input, e.g.,\n",
        "\n",
        "\\begin{equation}\n",
        "net_{y_1} = w_{1,1} x_1 + w_{1,2} x_2 + \\underbrace{-4}_\\text{bias term}\n",
        "\\end{equation}\n",
        "\n",
        "If we substitute in this new definition for $net_{y_1}$, our new output value is\n",
        "\n",
        "\\begin{equation}\n",
        "y_1 = \\frac{1}{1+e^{(-(w_{1,1} x_1 + w_{1,2} x_2 - 4))}} = \\frac{1}{1+e^{(4)}} = 0.0180\n",
        "\\end{equation}\n",
        "\n",
        "Since the sigmmoidal activation function is bounded between 0 and 1, its activation will never be exactly 0 or 1. However, we can get it as close to 0 as possible in order to minimize the error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMT7uGEET_gm"
      },
      "source": [
        "#### Learning the bias term"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JBDR1qlUXB4"
      },
      "source": [
        "What if we could teach the network to find the right bias to the output unit $y_j$? As shown above, the bias is just an additional term in the net input of a unit. We can therefore treat it as a separate input unit with value $b_{y_j}$ with a weight $w_{j}$ that projects to the output unit $y_j$:\n",
        "\n",
        "<p align='center'><img src='https://github.com/jmasis/nntutorialmentaleffort2021/blob/326079df67a7bfe602d30d6aee246b64f53b0f39/2LNetworkBias.png?raw=True'\n",
        "alt=\"2-layer network with bias unit\" width=600px height=auto/></p>\n",
        "\n",
        "The net input of $y_j$ then amounts to\n",
        "\n",
        "\\begin{equation}\n",
        "net_{y_1} = w_{1,1} x_1 + w_{1,2} x_2 + \\underbrace{b_{y_j} w_{j}}_\\text{bias term}\n",
        "\\end{equation}\n",
        "\n",
        "Now that we have expressed the bias term as another unit with a projection weight to $y_j$, we scan imply set the bias input unit $b_{y_j} = 1$ for every input pattern and let the network learn its weight $w_{j,b}$ in order to find the optimal bias of the network:\n",
        "\n",
        "\\begin{equation}\n",
        "w_{j,b}^{t+1} = \\underbrace{w_{j,b}^t}_\\text{current weight} + \\underbrace{\\Delta w_{j,b}^t}_\\text{weight change}\n",
        "\\end{equation}\n",
        "\n",
        "where\n",
        "\n",
        "\\begin{equation}\n",
        "\\Delta w_{j,b}^t = - \\underbrace{\\alpha}_\\text{learning rate} \\cdot\\underbrace{\\frac{\\partial E_{y_1}^t}{\\partial w_{j,b}^t}}_\\text{derivative w.r.t. bias}\n",
        "\\end{equation}\n",
        "\n",
        "For simplicity in Exercise 1.5 below, refer to $w_{j,b}^t$ as $\\verb|bias|$ and to $\\Delta w_{j,b}^t$ as $\\verb|delta_bias|$ in the code. Note that any unit (except input units) in the network can have a bias unit. In fact, it is usually the case that every unit of a layer in a network (except input layers) have their own bias units.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUHS1NOKV92C"
      },
      "source": [
        "### Exercise 1.5\n",
        "\n",
        "\n",
        "\n",
        "1.   With a pen and paper, use the chain rule in order to figure out how to compute the partial derivative of the error with respect to the bias.\n",
        "2.   Modify `train2LayerNetwork` to implement a bias unit on the output unit, as well as a mechanism for learning the weight of that bias unit. Once you have modified the code, run the function with `bias = True`.\n",
        "3.   Check whether implementing a bias unit improves learning on the AND & OR rule compared to having no bias.\n",
        "4.   Test the network's ability to learn the XOR rule. Why does it still have trouble learning the rule? How could we modify the network even further in order to make it learn the XOR rule?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NvMcjEkHhND"
      },
      "source": [
        "### The XOR Problem\n",
        "\n",
        "Why can the network not learn the XOR rule? To gain an intuition for this problem, you can think of our 2-layer neural network as a \"linear separator\", that classifies the inputs into 0 (red circle) or 1 (blue star), by linearly separating these patterns (formally, this corresponds to the dot product operation when computing the net input of our output unit). This works well for the AND and OR problem, but not so well for the XOR problem.\n",
        "\n",
        "<p align='center'><img src='https://raw.githubusercontent.com/jmasis/nntutorialmentaleffort2021/main/xor_problem.png'\n",
        "alt=\"2-layer network with bias unit\" width=600px height=auto/></p>\n",
        "\n",
        "We may apply a non-linear activation function to the output of our network, but it doesn't help us \"warp\" the input space in a non-linear manner. To do so, we need another, non-linear intermediate/hidden layer that can help us learn a non-linear transformation of the input space.\n",
        "\n",
        "\n",
        "<p align='center'><img src='https://raw.githubusercontent.com/jmasis/nntutorialmentaleffort2021/main/xor_solution2.png'\n",
        "alt=\"2-layer network with bias unit\" width=200px height=auto/></p>\n",
        "\n",
        "*Fun fact*: Back in the day, when researchers did not know how to train multi-layer non-linear networks, Minsky & Papert published a book called \"Perceptrons\". The authors showed through a set of analyses that simple, two-layer networks (or multi-layer linear networks) were unable to learn the XOR problem. This led to a pessimistic view on the capability of neural networks, and led researchers to focus on other approaches to AI, in particular symbolic systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRE5s7bYXOpV"
      },
      "source": [
        "## Overcoming the AI Winter: Three-Layer Neural Network & Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLI9-pfNXX0F"
      },
      "source": [
        "Now we will apply what we learned to a proper *deep neural network* (anything with more than 2 layers is technically a deep net! :)\n",
        "\n",
        "Recall that earlier we learned that all the steps we applied both in the forward and backward passes could be applied formulaically to any number of layers, not just 2.\n",
        "\n",
        "As such, take a deep breath, because you already know everything you need to know in order to implement a three-layer neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8AYp4UjYWga"
      },
      "source": [
        "### Defining a three-layer network\n",
        "\n",
        "There are many ways we could define a three-layer network. In our case, we are going to keep our input and output layers the same, and just add a layer in between them that we will call a **hidden layer**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji2hlyJcY9Z1"
      },
      "source": [
        "#### Hidden layer, what's that?\n",
        "\n",
        "A hidden layer is just a fancy term for a layer that is technically not visible to us, meaning it is neither the input (which we know), or the output (which we measure).\n",
        "\n",
        "We can probe into the network to see what the values of the hidden layer(s) are, but they might be hard to understand. In fact, as networks get bigger and more complex, they become *very* difficult to understand, which is why studying deep neural networks is such an active area of research!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evUhKIS0Zvcf"
      },
      "source": [
        "#### Network architecture\n",
        "\n",
        "Our network takes in 2 input units, spits out 1 output unit, and will have 3 **hidden units**. We will also add bias terms for the hidden layer and the output layer (as before).  \n",
        "\n",
        "<p align='center'><img src='https://raw.githubusercontent.com/jmasis/nntutorialmentaleffort2021/main/3_layer_network.png'\n",
        "alt=\"3-layer network with bias units\" width=500px height=auto/></p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEDZnetDaMgW"
      },
      "source": [
        "#### Defining the network mathematically\n",
        "\n",
        "*(<ins>Note</ins>: we will use linear algebra notation to define this 3-layer network because otherwise there would be too many equations. If you are not familiar with notation like this, do not worry! Ignore things like the uppercase$^T$ (\"transpose\"), and think of everything just as multiplication and addition, which it is. However, note that, in algebra notation, order matters. This means, e.g., that $Wx$ does not equal $xW$ (keep this in mind for Exercise 2.1).*\n",
        "\n",
        "##### **Connecting the input layer to the hidden layer**\n",
        "\n",
        "As before, our network takes in some input activity (2 binary values), which we will define as a vector $\\bf{x}$ of size (2 x 1).\n",
        "\n",
        "The vector $\\mathbf{x}$ will now be connected to the 3-unit hidden layer, defined by a vector $\\mathbf{h}$ of size (3 x 1).\n",
        "\n",
        "The connection between the vectors $\\mathbf{x}$ and $\\mathbf{h}$ will be a weight matrix $\\mathbf{W}_{h,x}$ of size (3 x 2). That is, the weight matrix $\\mathbf{W}_{h,x}$ has 3 rows corresponding to the 3 hidden units and 2 columns corresponding to the two output units.\n",
        "\n",
        "In addition, we will add a bias term (which we set to 1) to each hidden unit, weighted by a weight matrix $\\mathbf{W}_{h,bias}$ of size (3 x 1).\n",
        "\n",
        "We can now define the net input to the hidden layer $\\mathbf{net}_{h}$ of size (3x1),\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{net}_{h} = \\mathbf{W}_{h,x} \\mathbf{x} + 1\\cdot\\mathbf{W}_{h,bias}\n",
        "\\end{equation}\n",
        "\n",
        "Here's an example of how we can express the weights projecting from the input layer to the hidden layer in the form of a matrix:\n",
        "\n",
        "<p align='center'><img src='https://raw.githubusercontent.com/jmasis/nntutorialmentaleffort2021/main/example_weight_matrix.png'\n",
        "alt=\"3-layer network with bias units\" width=400px height=auto/></p>\n",
        "\n",
        "In this case, $\\mathbf{W}_{h,x} = \\begin{bmatrix} 0.1 & 0.4 \\\\ 0.2 & 0.5 \\\\ 0.3 & 0.6 \\end{bmatrix}$ and $\\mathbf{W}_{h,bias} = \\begin{bmatrix} -1 \\\\ -2 \\\\ -3 \\end{bmatrix}$. Let's assume the input pattern $\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$. We can compute the net input of the hidden layer as follows:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{net}_{h} &= \\begin{bmatrix} 0.1 & 0.4 \\\\ 0.2 & 0.5 \\\\ 0.3 & 0.6 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 1\\cdot \\begin{bmatrix} -1 \\\\ -2 \\\\ -3 \\end{bmatrix}\\\\\n",
        "\\mathbf{net}_{h} &= \\begin{bmatrix} 0.5 \\\\ 0.7 \\\\ 0.9 \\end{bmatrix} + \\begin{bmatrix} -1 \\\\ -2 \\\\ -3 \\end{bmatrix}\\\\\n",
        "\\mathbf{net}_{h} &= \\begin{bmatrix} -0.5 \\\\ -1.3 \\\\ -2.1 \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Our hidden layer will also have a logistic activation function, so its activation $\\mathbf{h}$ of size (3 x 1) is\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{h} = \\frac{1}{1 + e^{-\\mathbf{net}_{h}}} \\approx \\begin{bmatrix} 0 \\\\ 0 \\\\ 0.1 \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "##### **Connecting the hidden layer to the output layer**\n",
        "\n",
        "Now, our output layer will be connected to our hidden layer in the same fashion as it was connected to the input layer in the 2-layer network.\n",
        "\n",
        "Our output layer is a 1-unit vector $\\mathbf{y}$ of size (1 x 1) (so just a number), connected to the hidden layer $\\mathbf{h}$ of size (3 x 1), by a weight matrix $\\mathbf{W}_{y,h}$ of size (1 x 3).\n",
        "\n",
        "In addition, we will add a bias term (which we set to 1) to the output unit, weighted by a weight matrix $\\mathbf{W}_{y,bias}$ of size (1 x 1) (so just a number too).\n",
        "\n",
        "We can now define the net input to the output layer $\\mathbf{net}_{y}$ of size (1x1),\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{net}_{y} = \\mathbf{W}_{y,h} \\mathbf{h} + 1\\cdot\\mathbf{W}_{y,bias}\n",
        "\\end{equation}\n",
        "\n",
        "Again, let's consider the following example weights projecting from the hidden to the output layer:\n",
        "\n",
        "<p align='center'><img src='https://raw.githubusercontent.com/jmasis/nntutorialmentaleffort2021/main/example_hidden_output.png'\n",
        "alt=\"3-layer network with bias units\" width=400px height=auto/></p>\n",
        "\n",
        "\n",
        "with $\\mathbf{W}_{y,h} = \\begin{bmatrix} 0.1  & 0.2  & 0.3 \\end{bmatrix}$ and $\\mathbf{W}_{y,bias} = \\begin{bmatrix} 4 \\end{bmatrix}$. With this setup, the net input of the output unit amounts to\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{net}_{y} = \\begin{bmatrix} 0.1  & 0.2  & 0.3 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 0.1 \\end{bmatrix} + \\begin{bmatrix} 4 \\end{bmatrix} = \\begin{bmatrix} 4.3 \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "And like before, we have a sigmoid activation function, so its activation $\\mathbf{y}$ of size (1 x 1) is\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{y} = \\frac{1}{1 + e^{-\\mathbf{net}_{y}}} = 0.9866\n",
        "\\end{equation}\n",
        "\n",
        "##### **Remarks**\n",
        "\n",
        "As you may see, adding a third layer was just repeating what we already learned in the 2-layer example. As networks get larger and fancier, we can change the activation functions and bias terms used for every layer, and how connected they are through the weight matrices.\n",
        "\n",
        "*You now have the building blocks to understand the principles behind most neural networks!*\n",
        "\n",
        "You may have heard of fancy names applied to some neural networks, but you should be able to understand more or less how they work now based on this workshop. In the case of \"recurrent neural networks,\" we just connect layers back to each other. For example, imagine that there were two hidden layers and the activity of the second one fed back into the first one in a loop. In the case of \"convolutional neural networks,\" we just apply a special mathematical function called a convolution (think: averaging with a sliding window) to connect a big layer with a much smaller layer. For example, imagine that the input layer was not just 2 numbers, but rather an image, so a matrix of 256 x 256 numbers. We could connect it to the much smaller hidden layer via a convolution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j_88YqI5yx5"
      },
      "source": [
        "#### Backpropagation in 3-layer network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqLNo59S5_7Q"
      },
      "source": [
        "##### **Determining which weights to learn**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTFEzcEZgeWI"
      },
      "source": [
        "Backpropagating over the 3-layer network follows the same rules we followed for the 2-layer network, we just have more weights to learn now.\n",
        "\n",
        "Specifically, we would like to learn four sets of weights,\n",
        "\n",
        "\n",
        "1.   $\\mathbf{W}_{h,x}$ : the weights between the input and hidden layers\n",
        "2.   $\\mathbf{W}_{h,bias}$ : the weights between the hidden layer bias and hidden layer\n",
        "3.   $\\mathbf{W}_{y,h}$ : the weights between the output and hidden layers\n",
        "4.   $\\mathbf{W}_{y, bias}$: the weight between the output layer bias and the output layer\n",
        "\n",
        "To do this, we will do as before, and take the derivative of the error function **with respect to each of those weights**. As such, we have four derivatives we need to calculate,\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial E_{\\mathbf{y}}^t}{\\partial \\mathbf{W}_{h,x}^t},\n",
        "\\frac{\\partial E_{\\mathbf{y}}^t}{\\partial \\mathbf{W}_{h,bias}^t},\n",
        "\\frac{\\partial E_{\\mathbf{y}}^t}{\\partial \\mathbf{W}_{y,h}^t},\n",
        "\\frac{\\partial E_{\\mathbf{y}}^t}{\\partial \\mathbf{W}_{y,bias}^t}\n",
        "\\end{equation}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmf63I_Z6EW1"
      },
      "source": [
        "##### **Chain rule, we meet again**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRkpdHta5klm"
      },
      "source": [
        "The next step is to use the chain rule on each of these to calculate the derivative. We can do this by following the functions back to each of the weights, as we did before.\n",
        "\n",
        "Let's begin with derivative of the error function with respect to the weights of the input to the hidden layer, $\\frac{\\partial E_{\\mathbf{y}}^t}{\\partial \\mathbf{W}_{h,x}^t}$. We have five nested functions connecting the error function to these weights:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "E_{\\mathbf{y}} &=& \\frac{1}{2}(\\mathbf{y} - \\mathbf{t})^2 \\\\\n",
        "\\mathbf{y} &=& \\frac{1}{1 + e^{-\\mathbf{net}_{y}}}\\\\\n",
        "\\mathbf{net}_{y} &=& \\mathbf{W}_{y,h} \\mathbf{h} + 1\\cdot\\mathbf{W}_{y,bias}\\\\\n",
        "\\mathbf{h} &=& \\frac{1}{1 + e^{-\\mathbf{net}_{h}}}\\\\\n",
        "\\mathbf{net}_{h} &=& \\mathbf{W}_{h,x} \\mathbf{x} + 1\\cdot\\mathbf{W}_{h,bias}\n",
        "\\end{eqnarray}\n",
        "\n",
        "We should therefore have five partial derivatives when we apply the chain rule,\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial E_{\\mathbf{y}}^t}{\\partial \\mathbf{W}_{h,x}^t} =\n",
        "\\frac{\\partial E_{\\mathbf{y}}^t}{\\partial \\mathbf{y}}\n",
        "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{net}_{y}}\n",
        "\\frac{\\partial \\mathbf{net}_{y}}{\\partial \\mathbf{h}}\n",
        "\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{net}_{h}}\n",
        "\\frac{\\partial \\mathbf{net}_{h}}{\\partial \\mathbf{W}_{h,x}^t}\n",
        "\\end{equation}\n",
        "\n",
        "For the derivative with respect to the bias weights of the hidden layer, $\\frac{\\partial E_{\\mathbf{y}}^t}{\\partial \\mathbf{W}_{h,bias}^t}$, we have the same nested functions as above, but we just need to change the last term,\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial E_{\\mathbf{y}}^t}{\\partial \\mathbf{W}_{h,bias}^t} =\n",
        "\\frac{\\partial E_{\\mathbf{y}}^t}{\\partial \\mathbf{y}}\n",
        "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{net}_{y}}\n",
        "\\frac{\\partial \\mathbf{net}_{y}}{\\partial \\mathbf{h}}\n",
        "\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{net}_{h}}\n",
        "\\frac{\\partial \\mathbf{net}_{h}}{\\partial \\mathbf{W}_{h,bias}^t}\n",
        "\\end{equation}\n",
        "\n",
        "For the derivative of the weights connecting the hidden layer and output layer, we just need to apply the chain rule to the first three equations, instead of all five,\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial E_{\\mathbf{y}}^t}{\\partial \\mathbf{W}_{y,h}^t} =\n",
        "\\frac{\\partial E_{\\mathbf{y}}^t}{\\partial \\mathbf{y}}\n",
        "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{net}_{y}}\n",
        "\\frac{\\partial \\mathbf{net}_{y}}{\\partial \\mathbf{W}_{y,h}^t}\n",
        "\\end{equation}\n",
        "\n",
        "And similarly for the bias on the output layer,\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial E_{\\mathbf{y}}^t}{\\partial \\mathbf{W}_{y,bias}^t} =\n",
        "\\frac{\\partial E_{\\mathbf{y}}^t}{\\partial \\mathbf{y}}\n",
        "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{net}_{y}}\n",
        "\\frac{\\partial \\mathbf{net}_{y}}{\\partial \\mathbf{W}_{y,bias}^t}\n",
        "\\end{equation}\n",
        "\n",
        "Phew! From here on out, we just need to calculate each of the derivatives separately. Notice that the first two partial derivatives are the same for all four expressions, they are:\n",
        "\n",
        "<!-- Verbose:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial E_{\\mathbf{y}}^t}{\\partial \\mathbf{y}} =  \n",
        "\\frac{\\partial}{\\partial \\mathbf{y}} 0.5(\\mathbf{y} - \\mathbf{t})^2 = (\\mathbf{y} - \\mathbf{t})\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{net}_{y}} = \\frac{\\partial}{\\partial \\mathbf{net}_{y}} \\frac{1}{1 + e^{-\\mathbf{net}_{y}}} = \\mathbf{y}(1 - \\mathbf{y})\n",
        "\\end{equation} -->\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial E_{\\mathbf{y}}^t}{\\partial \\mathbf{y}} = (\\mathbf{y} - \\mathbf{t})\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{net}_{y}} = \\mathbf{y}(1 - \\mathbf{y})\n",
        "\\end{equation}\n",
        "\n",
        "The rest of the partial derivatives we need are:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial \\mathbf{net}_{y}}{\\partial \\mathbf{h}} &=& \\mathbf{W}_{y,h}^T \\\\\n",
        "\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{net}_{h}} &=& \\mathbf{h}(1 - \\mathbf{h}) \\\\\n",
        "\\frac{\\partial \\mathbf{net}_{h}}{\\partial \\mathbf{W}_{h,x}^t} &=& \\mathbf{x^T} \\\\\n",
        "\\frac{\\partial \\mathbf{net}_{y}}{\\partial \\mathbf{W}_{y,h}^t} &=& \\mathbf{h^T} \\\\\n",
        "\\frac{\\partial \\mathbf{net}_{h}}{\\partial \\mathbf{W}_{h,bias}^t} &=& 1 \\\\\n",
        "\\frac{\\partial \\mathbf{net}_{y}}{\\partial \\mathbf{W}_{y,bias}^t} &=& 1 \\\\\n",
        "\\end{eqnarray}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGg_DeHP5rSr"
      },
      "source": [
        "##### **Updating the weights**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1mok3GS6Jr1"
      },
      "source": [
        "Recall from earlier, that we update the weights by adding a small change,\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{W}^{t+1} = \\underbrace{\\mathbf{W}^t}_\\text{current weight} + \\underbrace{\\Delta \\mathbf{W}^t}_\\text{weight change}\n",
        "\\end{equation}\n",
        "\n",
        "where that small change is the learning rate $\\alpha$ times the partial derivative of the error with respect to those weights,\n",
        "\n",
        "\\begin{equation}\n",
        "\\Delta \\mathbf{W}^t = - \\underbrace{\\alpha}_\\text{learning rate} \\cdot\\underbrace{\\frac{\\partial E_{\\mathbf{y}}^t}{\\partial \\mathbf{W}^t}}_\\text{derivative w.r.t. some weights}\n",
        "\\end{equation}\n",
        "\n",
        "We can put all the pieces together in order to compute the weight changes for all four sets of weights,\n",
        "\n",
        "\\begin{equation}\n",
        "\\Delta \\mathbf{W}_{h,x}^t = - \\alpha (\\mathbf{y} - \\mathbf{t}) \\mathbf{y} (1 - \\mathbf{y}) \\mathbf{W}_{y,h}^T \\mathbf{h} (1 - \\mathbf{h})\\mathbf{x^T}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\Delta \\mathbf{W}_{h,bias}^t = - \\alpha (\\mathbf{y} - \\mathbf{t}) \\mathbf{y} (1 - \\mathbf{y}) \\mathbf{W}_{y,h}^T \\mathbf{h} (1 - \\mathbf{h})\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\Delta \\mathbf{W}_{y,h}^t = - \\alpha (\\mathbf{y} - \\mathbf{t}) \\mathbf{y} (1 - \\mathbf{y}) \\mathbf{h^T}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\Delta \\mathbf{W}_{y,bias}^t = - \\alpha (\\mathbf{y} - \\mathbf{t}) \\mathbf{y} (1 - \\mathbf{y})\n",
        "\\end{equation}\n",
        "\n",
        "*We did it!* Now we can write a function to simulate the 3-layer network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BMmoiRuc9bp"
      },
      "source": [
        "### Exercise 2.1\n",
        "\n",
        "The function `train3LayerNetwork` initializes a 3-layer neural network with 2 input units and 1 output unit (like in the example above). Your task is to fill in the missing code for the computation of its output (forward pass), as well as the missing code for network training (backward pass). The missing code is marked with '`...`'.\n",
        "\n",
        "Once you completed the code, run the simulation below. The simulation will output for each learning epoch (1 forward pass and 1 backward pass on every training pattern) the mean squared error across all training patterns. If your implementation is correct, then the network should be able to learn the XOR task.\n",
        "\n",
        "Once the network learns the XOR task, answer the following questions:\n",
        "\n",
        "*   Compare the learning curves for the AND/OR training patterns against the learning curve for the XOR training pattern. What do you observe? Can you explain the difference in the learning curves?\n",
        "*   Change the weight initialization from $W_{h,x},W_{y,h} \\sim U(0,1)$ to $W_{h,x},W_{y,h} \\sim U(0,0.1)$ and re-run the simulation. What happens? Now change the number of maximum learning epochs from 2,000 to 20,000. Can you explain why the difference in learning dynamics between the two weight initializations?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBDlvXFSqXvx"
      },
      "source": [
        "#### Define function 'train3LayerNework'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj0fhXpidAP3"
      },
      "source": [
        "def train3LayerNetwork(input_patterns, output_patterns, learning_rate, MSE_threshold, num_epochs):\n",
        "    ### network initialization ###\n",
        "\n",
        "    # let's define the number input and output units as a function of the dimension of the input and output patterns respectively\n",
        "    NInputUnits = input_patterns.shape[1]\n",
        "    NOutputUnits = output_patterns.shape[1]\n",
        "    # set number of hidden units to 3\n",
        "    NHiddenUnits = 3\n",
        "\n",
        "    # now we can intialize the two layers of the network...\n",
        "    x = np.zeros((1, NInputUnits))  # input layer\n",
        "    h = np.zeros((1, NHiddenUnits))  # hidden layer\n",
        "    y = np.zeros((1, NOutputUnits))  # output layer\n",
        "\n",
        "    # let's also log the error of the network\n",
        "    MSE_log = np.zeros((1, num_epochs))\n",
        "\n",
        "    # initialize weights from input layer to the hidden layer\n",
        "    W_hx = np.random.uniform(0, 1, (h.shape[1], x.shape[1]))\n",
        "\n",
        "    # initialize weights from hidden layer to the output layer\n",
        "    W_yh = np.random.uniform(0, 1, (y.shape[1], h.shape[1]))\n",
        "\n",
        "    # initialize weights from bias unit to hidden layer and output layer\n",
        "    W_hbias = np.random.uniform(0, 1, (h.shape[1], 1))\n",
        "    W_ybias = np.random.uniform(0, 1, (y.shape[1], 1))\n",
        "\n",
        "    W_hbias[:, 0] = -2\n",
        "    W_ybias[:, 0] = -2\n",
        "\n",
        "    ### network training ###\n",
        "\n",
        "    # the network will be trained in epochs.\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # initialize mean squared error log for all patterns\n",
        "        MSE_patterns = np.zeros((output_patterns.shape[0],))\n",
        "\n",
        "        # shuffle input patterns\n",
        "        # np.random.shuffle(input_patterns)\n",
        "\n",
        "        # within each training epoch, we will loop through every training pattern.\n",
        "        for pattern in range(input_patterns.shape[0]):\n",
        "            # FORWARD PASS #\n",
        "\n",
        "            # assign values to input layer\n",
        "            x = np.empty((2, 1))\n",
        "            x[:,0] = input_patterns[pattern,]\n",
        "\n",
        "            # compute net input of hidden layer\n",
        "            hbias = 1 * W_hbias\n",
        "            h_net = ... # YOUR CODE (hint: use np.dot)\n",
        "\n",
        "            # compute activation of hidden layer using sigmoidal activation function\n",
        "            h = ... # YOUR CODE (hint: use np.exp)\n",
        "\n",
        "            # compute net input of output layer\n",
        "            ybias = 1 * W_ybias\n",
        "            y_net = ...  # YOUR CODE (hint: use np.dot)\n",
        "\n",
        "            # compute activation of output layer using sigmoidal activation function\n",
        "            y = ...  # YOUR CODE (hint: use np.exp)\n",
        "\n",
        "            # ERROR BACKPROPAGATION #\n",
        "\n",
        "            # compute the mean squared error of the output with respect to the correct training pattern\n",
        "            MSE_patterns[pattern] = (y - output_patterns[pattern,]) ** 2\n",
        "\n",
        "            # DERIVATIVES\n",
        "\n",
        "            # compute derivative of the error with respect to the output unit activation\n",
        "            dError_dAct_y = (y - output_patterns[pattern,])\n",
        "\n",
        "            # compute derivative of output unit activation with respect to it's net input.\n",
        "            dAct_y_dNet_y = y * (1 - y)\n",
        "\n",
        "            # compute derivative of output unit activation with respect to the net input\n",
        "            dError_dNet_y = dError_dAct_y * dAct_y_dNet_y\n",
        "\n",
        "            # compute derivative of the output unit's net input with respect to the hidden unit activations\n",
        "            dNet_y_d_Act_h = W_yh.T\n",
        "\n",
        "            # compute derivative of the hidden unit activations with respect to the hidden units' net input\n",
        "            dAct_h_dNet_h = h * (1 - h)\n",
        "\n",
        "            # compute derivative of the error with respect to the hidden units' net input\n",
        "            dError_dNet_h = dError_dNet_y * np.multiply(dNet_y_d_Act_h, dAct_h_dNet_h)\n",
        "\n",
        "            # compute derivative of the hidden units' net input with respect to the input-hidden weights\n",
        "            dNet_h_dx = x.T\n",
        "\n",
        "            # compute the derivative of the output unit's net input with respect to hidden-output weights\n",
        "            dNet_h_dW_yh = h.T\n",
        "\n",
        "            # compute the derivative of the net input of the output unit with respect to it's weights to the bias unit\n",
        "            dNet_y_dW_ybias = 1\n",
        "\n",
        "            # compute the derivative of the net input of the hidden units with respect to their weights to the bias unit\n",
        "            dNet_h_dW_hbias = 1\n",
        "\n",
        "            # WEIGHT UPDATES\n",
        "\n",
        "            # compute weight adjustments from the hidden layer to the output layer\n",
        "            delta_W_yh = ... # YOUR CODE\n",
        "\n",
        "            # compute weight adjustments from the bias unit to the output layer\n",
        "            delta_W_ybias = ... # YOUR CODE\n",
        "\n",
        "            # compute weight adjustments from the input layer to the hidden layer\n",
        "            delta_W_hx = ... # YOUR CODE (hint: use dError_dNet_h)\n",
        "\n",
        "            # compute weight adjustments from the bias unit to the hidden layer\n",
        "            delta_W_hbias = ... # YOUR CODE (hint: use dError_dNet_h)\n",
        "\n",
        "            # adjust weights to output layer\n",
        "            W_yh = W_yh - ... # YOUR CODE\n",
        "            W_ybias = W_ybias - ... # YOUR CODE\n",
        "\n",
        "            # adjust weights to hidden layer\n",
        "            W_hx = W_hx - ... # YOUR CODE\n",
        "            W_hbias = W_hbias - ... # YOUR CODE\n",
        "\n",
        "        # log mean squared error for current epoch\n",
        "        MSE_log[0, epoch] = np.sum(MSE_patterns) / MSE_patterns.size\n",
        "\n",
        "        # break if we error threshold is reached\n",
        "        if MSE_log[0, epoch] < MSE_threshold:\n",
        "            break\n",
        "\n",
        "    # plot the error function\n",
        "    plt.plot(MSE_log[0,], '-b')\n",
        "    plt.xlim(0, num_epochs)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylim(0.001, 0.3)\n",
        "    plt.yticks([0.01, 0.05, 0.1, 0.15, 0.2, 0.25])\n",
        "    plt.ylabel('MSE')\n",
        "    plt.title('MSE as a function of training epoch')\n",
        "    sns.despine(trim=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu7QVBA5dA8M"
      },
      "source": [
        "#### Run simulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "C2gbsc7lHn0S",
        "outputId": "fd784b99-7c36-49c6-c83e-109f5f488274"
      },
      "source": [
        "# define parameters\n",
        "input = input_patterns\n",
        "output_pattern = output_patterns_XOR # output_patterns_OR # output_patterns_AND\n",
        "learning_rate = 0.5 # learning rate\n",
        "MSE_threshold = 0.01 # the training stops once the MSE is below this value\n",
        "num_epochs = 2000 # maximum number of epochs to train\n",
        "\n",
        "# run the network\n",
        "train3LayerNetwork(input, output_pattern, learning_rate, MSE_threshold, num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZlOvyOa7Ext"
      },
      "source": [
        "#### **Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYYlfQcudzwU"
      },
      "source": [
        "def train3LayerNetwork(input_patterns, output_patterns, learning_rate, MSE_threshold, num_epochs):\n",
        "    ### network initialization ###\n",
        "\n",
        "    # let's define the number input and output units as a function of the dimension of the input and output patterns respectively\n",
        "    NInputUnits = input_patterns.shape[1]\n",
        "    NOutputUnits = output_patterns.shape[1]\n",
        "    # set number of hidden units to 3\n",
        "    NHiddenUnits = 3\n",
        "\n",
        "    # now we can intialize the two layers of the network...\n",
        "    x = np.zeros((1, NInputUnits))  # input layer\n",
        "    h = np.zeros((1, NHiddenUnits))  # hidden layer\n",
        "    y = np.zeros((1, NOutputUnits))  # output layer\n",
        "\n",
        "    # let's also log the error of the network\n",
        "    MSE_log = np.zeros((1, num_epochs))\n",
        "\n",
        "    # initialize weights from input layer to the hidden layer\n",
        "    W_hx = np.random.uniform(0, 1, (h.shape[1], x.shape[1]))\n",
        "\n",
        "    # initialize weights from hidden layer to the output layer\n",
        "    W_yh = np.random.uniform(0, 1, (y.shape[1], h.shape[1]))\n",
        "\n",
        "    # initialize weights from bias unit to hidden layer and output layer\n",
        "    W_hbias = np.random.uniform(0, 1, (h.shape[1], 1))\n",
        "    W_ybias = np.random.uniform(0, 1, (y.shape[1], 1))\n",
        "\n",
        "    W_hbias[:, 0] = -2\n",
        "    W_ybias[:, 0] = -2\n",
        "\n",
        "    ### network training ###\n",
        "\n",
        "    # the network will be trained in epochs.\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # initialize mean squared error log for all patterns\n",
        "        MSE_patterns = np.zeros((output_patterns.shape[0],))\n",
        "\n",
        "        # shuffle input patterns\n",
        "        # np.random.shuffle(input_patterns)\n",
        "\n",
        "        # within each training epoch, we will loop through every training pattern.\n",
        "        for pattern in range(input_patterns.shape[0]):\n",
        "            # FORWARD PASS #\n",
        "\n",
        "            # assign values to input layer\n",
        "            x = np.empty((2, 1))\n",
        "            x[:,0] = input_patterns[pattern,]\n",
        "\n",
        "            # compute net input of hidden layer\n",
        "            hbias = 1 * W_hbias\n",
        "            h_net = np.dot(W_hx, x) + hbias # YOUR CODE (hint: use np.dot)\n",
        "\n",
        "            # compute activation of hidden layer using sigmoidal activation function\n",
        "            h = 1 / (np.exp(- (h_net)) + 1)  # YOUR CODE (hint: use np.exp)\n",
        "\n",
        "            # compute net input of output layer\n",
        "            ybias = 1 * W_ybias\n",
        "            y_net = np.dot(W_yh, h) + ybias  # YOUR CODE (hint: use np.dot)\n",
        "\n",
        "            # compute activation of output layer using sigmoidal activation function\n",
        "            y = 1 / (np.exp(- (y_net)) + 1)  # YOUR CODE (hint: use np.exp)\n",
        "\n",
        "            # ERROR BACKPROPAGATION #\n",
        "\n",
        "            # compute the mean squared error of the output with respect to the correct training pattern\n",
        "            MSE_patterns[pattern] = (y - output_patterns[pattern,]) ** 2\n",
        "\n",
        "            # DERIVATIVES\n",
        "\n",
        "            # compute derivative of the error with respect to the output unit activation\n",
        "            dError_dAct_y = (y - output_patterns[pattern,])\n",
        "\n",
        "            # compute derivative of output unit activation with respect to it's net input.\n",
        "            dAct_y_dNet_y = y * (1 - y)\n",
        "\n",
        "            # compute derivative of output unit activation with respect to the net input\n",
        "            dError_dNet_y = dError_dAct_y * dAct_y_dNet_y\n",
        "\n",
        "            # compute derivative of the output unit's net input with respect to the hidden unit activations\n",
        "            dNet_y_d_Act_h = W_yh.T\n",
        "\n",
        "            # compute derivative of the hidden unit activations with respect to the hidden units' net input\n",
        "            dAct_h_dNet_h = h * (1 - h)\n",
        "\n",
        "            # compute derivative of the error with respect to the hidden units' net input\n",
        "            dError_dNet_h = dError_dNet_y * np.multiply(dNet_y_d_Act_h, dAct_h_dNet_h)\n",
        "\n",
        "            # compute derivative of the hidden units' net input with respect to the input-hidden weights\n",
        "            dNet_h_dx = x.T\n",
        "\n",
        "            # compute the derivative of the output unit's net input with respect to hidden-output weights\n",
        "            dNet_h_dW_yh = h.T\n",
        "\n",
        "            # compute the derivative of the net input of the output unit with respect to it's weights to the bias unit\n",
        "            dNet_y_dW_ybias = 1\n",
        "\n",
        "            # compute the derivative of the net input of the hidden units with respect to their weights to the bias unit\n",
        "            dNet_h_dW_hbias = 1\n",
        "\n",
        "            # WEIGHT UPDATES\n",
        "\n",
        "            # compute weight adjustments from the hidden layer to the output layer\n",
        "            delta_W_yh = dError_dNet_y * dNet_h_dW_yh # YOUR CODE\n",
        "\n",
        "            # compute weight adjustments from the bias unit to the output layer\n",
        "            delta_W_ybias = dError_dNet_y * dNet_y_dW_ybias # YOUR CODE\n",
        "\n",
        "            # compute weight adjustments from the input layer to the hidden layer\n",
        "            delta_W_hx = dError_dNet_h * dNet_h_dx # YOUR CODE (hint: use dError_dNet_h)\n",
        "\n",
        "            # compute weight adjustments from the bias unit to the hidden layer\n",
        "            delta_W_hbias = dError_dNet_h * dNet_h_dW_hbias # YOUR CODE (hint: use dError_dNet_h)\n",
        "\n",
        "            # adjust weights to output layer\n",
        "            W_yh = W_yh - learning_rate * delta_W_yh # YOUR CODE\n",
        "            W_ybias = W_ybias - learning_rate * delta_W_ybias # YOUR CODE\n",
        "\n",
        "            # adjust weights to hidden layer\n",
        "            W_hx = W_hx - learning_rate * delta_W_hx # YOUR CODE\n",
        "            W_hbias = W_hbias - learning_rate * delta_W_hbias # YOUR CODE\n",
        "\n",
        "        # log mean squared error for current epoch\n",
        "        MSE_log[0, epoch] = np.sum(MSE_patterns) / MSE_patterns.size\n",
        "\n",
        "        # break if we error threshold is reached\n",
        "        if MSE_log[0, epoch] < MSE_threshold:\n",
        "            break\n",
        "\n",
        "    # plot the error function\n",
        "    plt.plot(MSE_log[0,], '-b')\n",
        "    plt.xlim(0, num_epochs)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylim(0.001, 0.3)\n",
        "    plt.yticks([0.01, 0.05, 0.1, 0.15, 0.2, 0.25])\n",
        "    plt.ylabel('MSE')\n",
        "    plt.title('MSE as a function of training epoch')\n",
        "    sns.despine(trim=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9lvCXRhBffF"
      },
      "source": [
        "**Answers**\n",
        "\n",
        "*   When comparing the learning curves for the AND/OR and the XOR condition, you should observe that the network requires fewer training iterations to learn AND/OR compared to XOR. Moreover, you may also observe a two-stage learning curve for the XOR condition in which learning slows down and then accelerates again. This reflects an additional adjustment of the input-hidden weights that is required to learn the XOR rule. First, the network learns to adjust the hidden-output weights to reduce the errorthis already sufficient to learn the OR/AND rule. However, to learn the XOR rule, the network needs to learn a non-linear mapping of the input space, thus, requiring an additional adjustment of the input-hidden weights (reflected in the second acceleration of the learning curve). Such stage-like transitions in the learning curve can explain developmental transitions in semantic cognition (Saxe, Ganguli, McClelland, 2019).\n",
        "*   When the network is initialized with smaller weights, its gradient updates end up being fairly small (due to the small derivatives). Thus, the network requires more iterations for training. This small exercise demonstrates that weight initialization matters. In fact, a good chunk of machine learning research is concerned with identifying good initialization conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPzM6CFGloxt"
      },
      "source": [
        "## A Neural Network Model of Mental Effort"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVtRHa8fBvxE"
      },
      "source": [
        "Now that you are equipped with a basic understanding of neural networks, you might wonder how you can use these kind of modeling too to understand mechanisms underlying mental effort? In this section, we will introduce a neural network model of mental effort, called the **Stroop model** (Cohen, Dunbar, & McClelland, 1990).\n",
        "\n",
        "### Performance in the Stroop Task as a Measure of Mental Effort\n",
        "\n",
        "The Stroop model operationalizes mental effort as cognitive control, that is, our ability to override habitual responses in order to align behavior with current task goals. For instance, you may be familiar with the so-called Stroop task in which participants have to name the color of a color word, e.g., say \"green\" in response to:\n",
        "\n",
        "<font color=\"green\" size=13> **RED** </font>\n",
        "\n",
        "We call this stimulus \"incongruent\" because the color and the word are associated with different responses. A congruent stimulus looks like this:\n",
        "\n",
        "<font color=\"green\" size=13> **GREEN** </font>\n",
        "\n",
        "Finally, there may be so-called neutral stimuli in which the participant has to respond to the color of a non-word:\n",
        "\n",
        "<font color=\"green\" size=13> **XXXX** </font>\n",
        "\n",
        "When studying human performance in the Stroop task, one can measure the error rate as a function of stimulus condition (neutral, incongruent, congruent), and as function of which task the participant performs (color naming or word reading). Human performance matches qualitatively this pattern:\n",
        "\n",
        "<p align='center'><img src='https://raw.githubusercontent.com/jmasis/nntutorialmentaleffort2021/main/stroop_results.png'\n",
        "alt=\"3-layer network with bias units\" width=300px height=auto/></p>\n",
        "\n",
        "That is, cognitive psychologists found that participants make fewer errors when being asked to respond to the word (performing the *word reading task*) compared to responding to the color (performing the *color naming task*). Thus, researchers assumed that word reading requires less mental effort (cognitive control) than color naming. In addition, participants make fewer errors when color naming to congruent relative to incongruent stimuli, irrespective of the task. Analogously, researchers hypothesized that congruent stimuli require less mental effort (cognitive control) than incongruent stimuli."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwe5nBKxLUhm"
      },
      "source": [
        "### The Stroop Model\n",
        "\n",
        "To explain these effects, Cohen et al. (1990) came up with a neural network model that operationalizes mental effort. The model looks like this (adapted from Figure 1 of Cohen et al, 1990):\n",
        "\n",
        "<p align='center'><img src='https://raw.githubusercontent.com/jmasis/nntutorialmentaleffort2021/main/stroop_neural_network_simple.png'\n",
        "alt=\"3-layer network with bias units\" width=500px height=auto/></p>\n",
        "\n",
        "\n",
        "The network consists of three input layers, two hidden layers and one output layer. The input layer on the left encodes the ink color of the stimulus. For instance, if the stimulus was shown in green red, we may set the input vector for the INK COLOR layer to $\\begin{bmatrix} 1 & 0 \\end{bmatrix}$. The input layer on the right encodes the current word. For instance, if the word would is \"GREEN\" then we can set the word input layer to $\\begin{bmatrix} 0 & 1 \\end{bmatrix}$. Finally, the input layer in the middle encodes the task to be performed on the two stimuli. For instance, if the participant would be asked to perform the color naming task, one would set the input units of the task demand layer to $\\begin{bmatrix} 1 & 0 \\end{bmatrix}$.\n",
        "\n",
        "The input layers project to two hidden layers, one that represents colors and another one that represents words. E.g., the first unit in the color hidden layer represents the color green. Note that inhibitory connections (negative weights) are depicted as dashed arrows. Finally, both hidden layers project to respective response units in the output layer.\n",
        "\n",
        "Note that the output layer receives inputs from two sources, the color hidden layer and the word hidden layer. Thus, if the information represented in the color and word hidden layers is incongruent, then the output layer will receive conflicting responses. However, an important feature of the Stroop model is that the hidden and output units are inhibited at rest. That is, these units receive a negative weight from a bias unit. Thus, a more accurate depiction of the network would be:\n",
        "\n",
        "<p align='center'><img src='https://raw.githubusercontent.com/jmasis/nntutorialmentaleffort2021/main/stroop_neural_network.png'\n",
        "alt=\"3-layer network with bias units\" width=500px height=auto/></p>\n",
        "\n",
        "where the green units are bias units. The inhibition is so strong that, without any input from the *TASK DEMAND* layer, the information from the color and word input layers would not \"make it through\" the hidden layers to the output layer. This is because adding a negative bias pushes the non-linear activation function to the right (see section \"The Role of Bias Units\" above).\n",
        "\n",
        "<p align='center'><img src='https://github.com/jmasis/nntutorialmentaleffort2021/blob/326079df67a7bfe602d30d6aee246b64f53b0f39/BiasEffect.png?raw=True'\n",
        "alt=\"Effect of bias on sigmoidal activation function\" width=400px height=auto/></p>\n",
        "\n",
        "As a consequence, small net inputs will evaluate to something close to zero. However, the cool part about the Stroop model is that the *TASK DEMAND* layer can provide additional net input to the hidden units, making it possible to disinhibit them. For instance, if we set the color naming unit to 1, then we can provide additonal net input to the color hidden units. This negates the negative bias, allowing information from the *INK COLOR* input layer to flow through to the *RESPONSE* layer. Conversely, if we leave the word reading unit in the *TASK DEMAND* layer at 0, then the information from the *WORD* layer cannot leak through. The output layer would then only receive input from the color hidden units, thus enabling the network to perform the color naming task. This provides a mechanism for mental effort: allocating mental effort to a task means to activate a task demand unit (i.e., set it to 1).\n",
        "\n",
        "Now lets look into an actual implementation of the Stroop model...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZi0gX9bQMYP"
      },
      "source": [
        "The following function \"**runStroopNetwork**\" implements the Stroop model. Note that the weights are denoted as \"W_source_layer_target_layer\". However, the weights are not yet definedthis is OK! Defining the weights will be part of an exercise that we will come to later. For now, let's just execute the function below and have a closer look..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi5Cc1dZ6K8K"
      },
      "source": [
        "# this function accepts a single input pattern for the color, word and task input layers\n",
        "# and produces a response at the output layer.\n",
        "def runStroopNetwork(color_input, word_input, task_input):\n",
        "\n",
        "    # compute activation of color hidden layer\n",
        "    net_hidden_color = np.dot(color_input, W_input_color_hidden_color) + \\\n",
        "                       np.dot(task_input, W_input_task_hidden_color) + \\\n",
        "                       bias\n",
        "\n",
        "    act_hidden_color = 1 / (1 + np.exp(-net_hidden_color))\n",
        "\n",
        "    # compute activation of word hidden layer\n",
        "    net_hidden_word = np.dot(word_input, W_input_word_hidden_word) + \\\n",
        "                      np.dot(task_input, W_input_task_hidden_word) + \\\n",
        "                      bias\n",
        "\n",
        "    act_hidden_word = 1 / (1 + np.exp(-net_hidden_word))\n",
        "\n",
        "    # compute activation of output layer\n",
        "    net_output = np.dot(act_hidden_color, W_hidden_color_output) + \\\n",
        "                 np.dot(act_hidden_word, W_hidden_word_output)\n",
        "\n",
        "    act_output = 1 / (1 + np.exp(-net_output))\n",
        "\n",
        "    # We apply the softmax function convert the output activation for each unit (response) into a probability\n",
        "    response_probability = np.exp(net_output) / np.sum(np.exp(net_output), axis=1)\n",
        "\n",
        "    return response_probability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWZ6AINRQ7_L"
      },
      "source": [
        "You may notice that the network is not very special, at least computationally: It performs the same net input integration, anf non-linear activation functions as the networks in the previous sections. There are only three things that make this network special compared to the network above:\n",
        "\n",
        "\n",
        "1.   It has more than one output unit.\n",
        "2.   It has multiple (3) input layers and multiple (2) hidden layers.\n",
        "3.   It uses a softmax function at the output layer.\n",
        "\n",
        "The first two points aren't really new, since they are simple extensions of the formalism introduced above; So let's have a look at the computation of the output layer...\n",
        "\n",
        "The response probabilities of the network are computed as follows:\n",
        "\n",
        "\\begin{align}\n",
        "P(\\text{red}) &= \\frac{e^{net_{\\text{output},red}}}{e^{net_{\\text{output},red}} + e^{net_{\\text{output},green}}} \\\\\n",
        "P(\\text{green}) = 1 - P(\\text{red}) &= \\frac{e^{net_{\\text{output},green}}}{e^{net_{\\text{output},red}} + e^{net_{\\text{output},green}}}\n",
        "\\end{align}\n",
        "\n",
        "This function is called the **softmax** function. All it does is that it forces the activities for both output units to sum up to 1. Thus, we can treat them as probabilities. The response probability for each unit is proportional to its respective net input, weighted by the net input of the other unit.\n",
        "\n",
        "OK, now that we have a Stroop model, we need an experiment environment. The following function \"**runStroopExperiment**\" probes the network's response for different Stroop conditions (see the code below). For now, let's just execute the function so we can call it later...\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub80CeDw6TtS"
      },
      "source": [
        "# computes the network's error rate across different conditions\n",
        "\n",
        "def runStroopExperiment():\n",
        "    # color naming, congruent stimulus\n",
        "    color_input = np.array([[1, 0]])    # color: red\n",
        "    word_input = np.array([[1, 0]])     # word: red\n",
        "    task_input = np.array([[1, 0]])     # task: color naming\n",
        "    cn_congruent = runStroopNetwork(color_input, word_input, task_input)\n",
        "\n",
        "    # color naming, incongruent stimulus\n",
        "    color_input = np.array([[1, 0]])    # color: red\n",
        "    word_input = np.array([[0, 1]])     # word: green\n",
        "    task_input = np.array([[1, 0]])     # task: color naming\n",
        "    cn_incongruent = runStroopNetwork(color_input, word_input, task_input)\n",
        "\n",
        "    # color naming, neutral stimulus\n",
        "    color_input = np.array([[1, 0]])    # color: red\n",
        "    word_input = np.array([[0, 0]])     # word: XXX\n",
        "    task_input = np.array([[1, 0]])     # task: color naming\n",
        "    cn_neutral = runStroopNetwork(color_input, word_input, task_input)\n",
        "\n",
        "    # word reading, congruent stimulus\n",
        "    color_input = np.array([[1, 0]])    # color: red\n",
        "    word_input = np.array([[1, 0]])     # word: red\n",
        "    task_input = np.array([[0, 1]])     # task: word reading\n",
        "    wr_congruent = runStroopNetwork(color_input, word_input, task_input)\n",
        "\n",
        "    # word reading, incongruent stimulus\n",
        "    color_input = np.array([[0, 1]])    # color: green\n",
        "    word_input = np.array([[1, 0]])     # word: red\n",
        "    task_input = np.array([[0, 1]])     # task: word reading\n",
        "    wr_incongruent = runStroopNetwork(color_input, word_input, task_input)\n",
        "\n",
        "    # word reading, neutral stimulus\n",
        "    color_input = np.array([[0, 0]])    # color: red\n",
        "    word_input = np.array([[1, 0]])     # word: red\n",
        "    task_input = np.array([[0, 1]])     # task: word reading\n",
        "    wr_neutral = runStroopNetwork(color_input, word_input, task_input)\n",
        "\n",
        "    # plot error rates for each condition (assuming \"red\" is the correct response)\n",
        "    cn_congruent_error_rate = 1 - cn_congruent[0, 0]\n",
        "    cn_incongruent_error_rate = 1 - cn_incongruent[0, 0]\n",
        "    cn_neutral_error_rate = 1 - cn_neutral[0, 0]\n",
        "    wr_congruent_error_rate = 1 - wr_congruent[0, 0]\n",
        "    wr_incongruent_error_rate = 1 - wr_incongruent[0, 0]\n",
        "    wr_neutral_error_rate = 1 - wr_neutral[0, 0]\n",
        "\n",
        "    return (cn_congruent_error_rate, cn_incongruent_error_rate, cn_neutral_error_rate,\n",
        "            wr_congruent_error_rate, wr_incongruent_error_rate, wr_neutral_error_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7b2PD24T9au"
      },
      "source": [
        "### Exercise 3.1\n",
        "\n",
        "Now that we have a Stroop model and a Stroop experiment, let's see how it behaves. The following code section defines the weights for the Stroop model and measures its performance. Execute the code block and see what happens.\n",
        "Does the network produce the expected behavior? Well, of course not, otherwise this wouldn't be an exercise. Now, can you adjust the weights of the Stroop model so that it produces behavior similar to the figure shown above?\n",
        "\n",
        "Open ended exercise: A major tenet of the Stroop model is that the word reading task requires less effort because it's processing path from the input to the output layer is stronger, as a consequence of training (lifelong practice on word reading as opposed to color naming). Wouldn't it be nice if we could show that this behavior results indeed from training? To do so, you may want to think tacke two extended exercises:\n",
        "\n",
        "\n",
        "1.   Apply the backpropagation algorithm to make the Stroop network trainable.\n",
        "2.   Train the Stroop network on a higher proportion of word reading conditions than color naming conditions. Can you replicate the behavioral pattern expected in the Stroop task?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZmKYcoDlwAH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "7752718c-5963-4cfd-dd7c-082451f42753"
      },
      "source": [
        "# STROOP NETWORK WEIGHTS\n",
        "\n",
        "bias = -4\n",
        "\n",
        "# weights projecting from color input layer to color hidden layer\n",
        "W_input_color_hidden_color = 2 * np.array([[1, -1],   # EXPERIMENT WITH THESE WEIGHTS\n",
        "                                             [-1, 1]])\n",
        "\n",
        "# weights projecting from color hidden layer to output layer\n",
        "W_hidden_color_output = 2 * np.array([[1, -1],    # EXPERIMENT WITH THESE WEIGHTS\n",
        "                                        [-1, 1]])\n",
        "\n",
        "# weights projecting from word input layer to word hidden layer\n",
        "W_input_word_hidden_word = 2 * np.array([[1, -1],   # EXPERIMENT WITH THESE WEIGHTS\n",
        "                                           [-1, 1]])\n",
        "\n",
        "# weights projecting from word hidden layer to output layer\n",
        "W_hidden_word_output = 2 * np.array([[1, -1],   # EXPERIMENT WITH THESE WEIGHTS\n",
        "                                      [-1, 1]])\n",
        "\n",
        "# weights projecting from task layer to color hidden layer\n",
        "W_input_task_hidden_color = 4.0 * np.array([[1, 1],   # EXPERIMENT WITH THESE WEIGHTS\n",
        "                                            [0, 0]])\n",
        "\n",
        "# weights projecting from task layer to word hidden layer\n",
        "W_input_task_hidden_word = 4.0 * np.array([[0, 0],    # EXPERIMENT WITH THESE WEIGHTS\n",
        "                                           [1, 1]])\n",
        "\n",
        "\n",
        "# PLOT THE NETWORK PERFORMANCE\n",
        "\n",
        "(cn_congruent_error_rate, cn_incongruent_error_rate, cn_neutral_error_rate,\n",
        "            wr_congruent_error_rate, wr_incongruent_error_rate, wr_neutral_error_rate) = runStroopExperiment()\n",
        "\n",
        "x_data = [0, 1, 2]\n",
        "y_data_col = [cn_neutral_error_rate * 100, cn_incongruent_error_rate * 100, cn_congruent_error_rate * 100]\n",
        "y_data_wrd = [wr_neutral_error_rate * 100, wr_incongruent_error_rate * 100, wr_congruent_error_rate * 100]\n",
        "x_limit = [-0.5, 2.5]\n",
        "y_limit = [0, 100]\n",
        "x_label = \"Condition\"\n",
        "y_label = \"Error Rate (%)\"\n",
        "legend = ('color naming', 'word reading')\n",
        "\n",
        "plt.plot(x_data, y_data_col, label=legend[0])\n",
        "plt.plot(x_data, y_data_wrd, '--', label=legend[1])\n",
        "plt.xlim(x_limit)\n",
        "plt.ylim(y_limit)\n",
        "plt.ylabel(y_label, fontsize=\"large\")\n",
        "plt.legend(loc=2, fontsize=\"large\")\n",
        "plt.xticks([0, 1, 2], ['Neutral', 'Incongruent', 'Congruent'],rotation=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFHu6yz8UnW5"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "nWCZyOMnUpw7",
        "outputId": "aec2d4f2-28b3-4105-daf9-9c66e476e7b7"
      },
      "source": [
        "# STROOP NETWORK WEIGHTS\n",
        "\n",
        "bias = -4\n",
        "\n",
        "# weights projecting from color input layer to color hidden layer\n",
        "W_input_color_hidden_color = 2.2 * np.array([[1, -1],\n",
        "                                             [-1, 1]])\n",
        "\n",
        "# weights projecting from color hidden layer to output layer\n",
        "W_hidden_color_output = 1.3 * np.array([[1, -1],\n",
        "                                        [-1, 1]])\n",
        "\n",
        "# weights projecting from word input layer to word hidden layer\n",
        "W_input_word_hidden_word = 2.6 * np.array([[1, -1],\n",
        "                                           [-1, 1]])\n",
        "\n",
        "# weights projecting from word hidden layer to output layer\n",
        "W_hidden_word_output = 2.5 * np.array([[1, -1],\n",
        "                                      [-1, 1]])\n",
        "\n",
        "# weights projecting from task layer to color hidden layer\n",
        "W_input_task_hidden_color = 4.0 * np.array([[1, 1],\n",
        "                                            [0, 0]])\n",
        "\n",
        "# weights projecting from task layer to word hidden layer\n",
        "W_input_task_hidden_word = 4.0 * np.array([[0, 0],\n",
        "                                           [1, 1]])\n",
        "\n",
        "\n",
        "# PLOT THE NETWORK PERFORMANCE\n",
        "\n",
        "(cn_congruent_error_rate, cn_incongruent_error_rate, cn_neutral_error_rate,\n",
        "            wr_congruent_error_rate, wr_incongruent_error_rate, wr_neutral_error_rate) = runStroopExperiment()\n",
        "\n",
        "x_data = [0, 1, 2]\n",
        "y_data_col = [cn_neutral_error_rate * 100, cn_incongruent_error_rate * 100, cn_congruent_error_rate * 100]\n",
        "y_data_wrd = [wr_neutral_error_rate * 100, wr_incongruent_error_rate * 100, wr_congruent_error_rate * 100]\n",
        "x_limit = [-0.5, 2.5]\n",
        "y_limit = [0, 100]\n",
        "x_label = \"Condition\"\n",
        "y_label = \"Error Rate (%)\"\n",
        "legend = ('color naming', 'word reading')\n",
        "\n",
        "plt.plot(x_data, y_data_col, label=legend[0])\n",
        "plt.plot(x_data, y_data_wrd, '--', label=legend[1])\n",
        "plt.xlim(x_limit)\n",
        "plt.ylim(y_limit)\n",
        "plt.ylabel(y_label, fontsize=\"large\")\n",
        "plt.legend(loc=2, fontsize=\"large\")\n",
        "plt.xticks([0, 1, 2], ['Neutral', 'Incongruent', 'Congruent'],rotation=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KbkL8Q2GAzX"
      },
      "source": [
        "# References and Further Reading\n",
        "\n",
        "#### General Introductions\n",
        "\n",
        "*   **Neural network models of human cognition**: McClelland, James L., David E. Rumelhart, and PDP Research Group. Parallel distributed processing. Vol. 2. Cambridge, MA: MIT press, 1986.\n",
        "*   **Biologically-inspired neural network models**: O'Reilly, R. C., & Munakata, Y. (2000). Computational explorations in cognitive neuroscience: Understanding the mind by simulating the brain. MIT press.\n",
        "*   **Machine learning**: Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press.\n",
        "\n",
        "\n",
        "#### Neural Networks as Models of Mental Effort\n",
        "*   **Cognitive Control & the Stroop Task**: Cohen, J. D., Dunbar, K., & McClelland, J. L. (1990). On the control of automatic processes: a parallel distributed processing account of the Stroop effect. Psychological review, 97(3), 332.\n",
        "*   **Cognitive Control & Conflict Monitoring**: Botvinick, M. M., Braver, T. S., Barch, D. M., Carter, C. S., & Cohen, J. D. (2001). Conflict monitoring and cognitive control. Psychological review, 108(3), 624.\n",
        "*   **Cognitive Control & Task Switching**: Gilbert, S. J., & Shallice, T. (2002). Task switching: A PDP model. Cognitive psychology, 44(3), 297-337.\n",
        "*   **Cognitive Control & Multitasking**: Musslick, S., & Cohen, J. D. (2021). Rationalizing constraints on the capacity for cognitive control. Trends in Cognitive Sciences. 25(9), 757775\n",
        "*   **Decision-Making**: Usher, M., & McClelland, J. L. (2001). The time course of perceptual choice: the leaky, competing accumulator model. Psychological review, 108(3), 550.\n",
        "\n",
        "#### Other References\n",
        "*   **Recommended tutorial for machine learning**: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
        "*   **Early criticism of neural networks**: Minksy, M., & Papert, S. (1969). Perceptrons: An Introduction to Computational Geometry. MIT. Press.\n",
        "*   **Semantic Cognition**: Rogers, T. T., & McClelland, J. L. (2004). Semantic cognition: A parallel distributed processing approach. MIT press.\n",
        "*   **Semantic Development**: Saxe, A. M., McClelland, J. L., & Ganguli, S. (2019). A mathematical theory of semantic development in deep neural networks. Proceedings of the National Academy of Sciences, 116(23), 11537-11546."
      ]
    }
  ]
}